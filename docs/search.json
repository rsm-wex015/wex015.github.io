[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wenxin(Wendy) Xu",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nWenxin Xu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nWenxin Xu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe use the dataset made public by the authors, consisting of 50,083 prior donors who were randomly assigned into different treatment groups. The dataset contains variables indicating treatment status, donation behavior, suggested amounts, demographics, and political/geographic characteristics.\n\n\nSample Overview\n\nObservations: 50,083\nTreatments:\n\nControl\nMatching grants: $1:$1, $2:$1, $3:$1\n\nKey Outcomes:\n\ngave: whether the donor gave anything\namount: donation amount (if any)\n\nExample Variables:\n\nsize: suggested donation size (Unstated, $50, $100, etc.)\nredcty: lives in a red county\nhomestate: same state as charity ### Load and Preview the Data\n\n\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe test three pre-treatment variables: - Months since last donation (mrm2) - Highest previous contribution (hpa) - Number of prior donations (freq)\nThese variables should be similar across treatment and control groups if randomization was successful. We perform both t-tests and linear regressions for robustness. The table below summarizes the results.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\nbalance_vars = {\n    'Months Since Last Donation': 'mrm2',\n    'Highest Previous Contribution': 'hpa',\n    'Number of Prior Donations': 'freq'\n}\n\n# T-tests and OLS regressions\nttest_results = {}\nols_results = {}\n\nfor label, var in balance_vars.items():\n    control_vals = df[df['treatment'] == 0][var].dropna()\n    treatment_vals = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = stats.ttest_ind(control_vals, treatment_vals, equal_var=False)\n    ttest_results[label] = {'t_stat': t_stat, 'p_val': p_val}\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n    ols_results[label] = {'coef': coef, 'p_val': pval}\n\n# Format for display\nbalance_summary = pd.DataFrame({\n    'Variable': list(balance_vars.keys()),\n    'T-test p-value': [ttest_results[v]['p_val'] for v in balance_vars],\n    'OLS coef on treatment': [ols_results[v]['coef'] for v in balance_vars],\n    'OLS p-value': [ols_results[v]['p_val'] for v in balance_vars]\n})\n\nbalance_summary\n\n\n\n\n\n\n\n\nVariable\nT-test p-value\nOLS coef on treatment\nOLS p-value\n\n\n\n\n0\nMonths Since Last Donation\n0.904855\n0.013686\n0.904886\n\n\n1\nHighest Previous Contribution\n0.331840\n0.637075\n0.345099\n\n\n2\nNumber of Prior Donations\n0.911740\n-0.011979\n0.911702\n\n\n\n\n\n\n\nThe balance test confirms that randomization worked as intended. Across the three pre-treatment variables — months since last donation, highest previous contribution, and number of prior donations — none show statistically significant differences between treatment and control groups at the 5% level. ﻿ This supports the internal validity of the experiment: any observed effects in giving behavior are unlikely to be due to baseline differences."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nMatching Donations Increase Giving Probability\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe compare the proportion of individuals who donated between the treatment group and the control group. This helps us understand whether the announcement of a matching gift increases the likelihood of donating.\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates by group\ndonation_rates = df.groupby('treatment')['gave'].mean()\ndonation_rates.index = ['Control', 'Treatment']\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(6, 4))\nbars = ax.bar(donation_rates.index, donation_rates.values, width=0.5, color=['#4C72B0', '#55A868'])\n\n# Add percentage labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.002,\n            f'{height:.2%}', ha='center', va='bottom', fontsize=10)\n\n# Formatting\nax.set_ylabel('Proportion Donated')\nax.set_title('Effect of Matching Gift on Donation Rate')\nax.set_ylim(0, max(donation_rates.values) + 0.03)\nax.grid(axis='y', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart shows that individuals in the treatment group donated at a rate of 2.20%, compared to 1.79% in the control group — an increase of approximately 23% in relative terms. This result suggests that the announcement of a matching gift offer increases donor engagement, likely because donors feel their contribution has greater impact.\nThis initial analysis replicates the paper’s key finding: matching incentives effectively nudge donors into action.\n\n\nStatistical Significance: T-test and Linear Regression\nWe test whether the observed difference in donation behavior is statistically significant, using both a t-test and a bivariate regression.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Group samples\ncontrol = df[df['treatment'] == 0]['gave']\ntreatment = df[df['treatment'] == 1]['gave']\n\n# T-test (unequal variance)\nt_stat, p_val = ttest_ind(treatment, control, equal_var=False)\n\n# OLS Regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\nreg_summary = model.summary()\nprint(t_stat, p_val, reg_summary)\nprint(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4f}\")\n\n3.2094621908279835 0.0013309823450914173                             OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        19:52:14   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-statistic: 3.2095, P-value: 0.0013\n\n\nBoth tests lead to the same conclusion:\nThe t-test indicates a statistically significant difference in mean donation rates between treatment and control (p-value &lt; 0.01). The regression coefficient on treatment is +0.0042, indicating that individuals in the treatment group were 0.42 percentage points more likely to donate, which is about a 23% increase over the control group mean of 1.79%. This finding is statistically significant (p = 0.002), and replicates the original result in Table 2a Panel A of Karlan and List (2007).\nBehavioral Insight When individuals are told that their contribution will be matched, they perceive their action as more impactful. This increases their willingness to give, even if the absolute probability of donation remains low. Matching gifts operate not only as a financial lever but also as a psychological motivator, increasing perceived effectiveness.\n\n\nProbit Model with Marginal Effects\nWhile the probit coefficient on treatment was 0.087, that number cannot be directly interpreted as a change in probability. So, we compute the marginal effect of treatment.\n\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment'])).fit()\nmarginal_effects = probit_model.get_margeff()\nmarginal_effects.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\nThe marginal effect of treatment is approximately 0.004, which means that receiving a matching offer increased the probability of donation by 0.4 percentage points — exactly what the original paper reports using a linear probability model. In this way, the Probit model gives us both a robust theoretical approach and a practically interpretable result."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo simulate the LLN, we:\n\nDraw 10,000 Bernoulli samples from each group\nCalculate the difference in outcomes for each simulated pair\nPlot the cumulative average of the differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn_draws = 10000\nnp.random.seed(42)\n\n# Simulate\ncontrol_draws = np.random.binomial(1, p_control, n_draws)\ntreatment_draws = np.random.binomial(1, p_treatment, n_draws)\n\n# Difference vector\ndifferences = treatment_draws - control_draws\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\ntrue_diff = p_treatment - p_control\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. of Differences', color='orange')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers Simulation: Convergence to True Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Difference in Donation Probability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe simulation shows that the cumulative average of the differences converges to the true value of 0.004, which is the expected treatment effect. This is a visual and intuitive demonstration of the Law of Large Numbers: with more data, our sample statistics converge to population parameters.\nThis validates the logic behind the t-test and confirms why large samples allow for reliable estimation of small effects — just like in Karlan & List (2007), where a small increase in giving (from 1.8% to 2.2%) was detected across a large sample.\n\n\nCentral Limit Theorem\nTo visualize the Central Limit Theorem, we simulate 1,000 experiments for each of four sample sizes: 50, 200, 500, and 1000. In each simulation, we:\n\nDraw n observations from a Bernoulli(p=0.018) distribution (control group)\nDraw n from a Bernoulli(p=0.022) distribution (treatment group)\nCalculate the difference in sample means (treatment - control)\nRepeat 1,000 times and plot the histogram of these average differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor idx, n in enumerate(sample_sizes):\n    avg_diffs = [] \n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n  \n    axes[idx].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[idx].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[idx].set_title(f\"Sample Size = {n}\")\n    axes[idx].set_xlabel(\"Avg. Difference (Treatment - Control)\")\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].legend()\n\nplt.suptitle(\"Central Limit Theorem Simulation: Sampling Distributions of Avg. Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that:\nAs sample size increases, the distribution of average differences becomes tighter and more symmetric For small sample sizes (e.g., n = 50), the histogram is more spread out and zero often appears near the center For larger sample sizes (n = 500 or 1000), the distribution concentrates around the true mean difference (~0.004), and zero moves closer to the edge or tail This behavior demonstrates the Central Limit Theorem in action:\nEven though the original data (Bernoulli) is highly skewed, the sampling distribution of the mean difference becomes approximately normal, and its variance shrinks with larger n. This validates why Karlan & List’s experiment was able to detect a small effect (2.2% vs 1.8%) — their large sample size ensured that sampling variation wouldn’t drown out the true effect."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Test",
    "section": "",
    "text": "Hello world!\nsummary(mtcars)"
  },
  {
    "objectID": "projects/project1/project1.html",
    "href": "projects/project1/project1.html",
    "title": "Wendy",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\nbalance_vars = {\n    'Months Since Last Donation': 'mrm2',\n    'Highest Previous Contribution': 'hpa',\n    'Number of Prior Donations': 'freq'\n}\n\n# T-tests and OLS regressions\nttest_results = {}\nols_results = {}\n\nfor label, var in balance_vars.items():\n    control_vals = df[df['treatment'] == 0][var].dropna()\n    treatment_vals = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = stats.ttest_ind(control_vals, treatment_vals, equal_var=False)\n    ttest_results[label] = {'t_stat': t_stat, 'p_val': p_val}\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n    ols_results[label] = {'coef': coef, 'p_val': pval}\n\n# Format for display\nbalance_summary = pd.DataFrame({\n    'Variable': list(balance_vars.keys()),\n    'T-test p-value': [ttest_results[v]['p_val'] for v in balance_vars],\n    'OLS coef on treatment': [ols_results[v]['coef'] for v in balance_vars],\n    'OLS p-value': [ols_results[v]['p_val'] for v in balance_vars]\n})\n\nbalance_summary\n\n\n\n\n\n\n\n\nVariable\nT-test p-value\nOLS coef on treatment\nOLS p-value\n\n\n\n\n0\nMonths Since Last Donation\n0.904855\n0.013686\n0.904886\n\n\n1\nHighest Previous Contribution\n0.331840\n0.637075\n0.345099\n\n\n2\nNumber of Prior Donations\n0.911740\n-0.011979\n0.911702\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = df.groupby('treatment')['gave'].mean()\ndonation_rates.index = ['Control', 'Treatment']\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(donation_rates.index, donation_rates.values, width=0.5)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates by Group')\n\n# Add percentage labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, height + 0.01,\n             f'{height:.2%}', ha='center', va='bottom')\n\nplt.ylim(0, max(donation_rates.values) + 0.05)\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# T-test\ncontrol_group = df[df['treatment'] == 0]['gave']\ntreatment_group = df[df['treatment'] == 1]['gave']\nt_stat, p_val = ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\nreg_summary = model.summary()\n\nt_stat, p_val, reg_summary\n\n(np.float64(3.2094621908279835),\n np.float64(0.0013309823450914173),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        15:47:55   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nimport statsmodels.api as sm\n\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment'])).fit()\n\n# 计算平均边际效应\nmarginal_effects = probit_model.get_margeff()\nmarginal_effects.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Only look at treatment group\ndf_match = df[df['treatment'] == 1]\n\n# Define match groups\ngave_1_1 = df_match[df_match['ratio'] == 1]['gave']\ngave_2_1 = df_match[df_match['ratio'] == 2]['gave']\ngave_3_1 = df_match[df_match['ratio'] == 3]['gave']\n\n# T-tests\nttest_2v1 = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 p-value:\", round(ttest_2v1.pvalue, 4))\nprint(\"3:1 vs 1:1 p-value:\", round(ttest_3v1.pvalue, 4))\n\n2:1 vs 1:1 p-value: 0.3345\n3:1 vs 1:1 p-value: 0.3101\n\n\n\nimport statsmodels.formula.api as smf\n\n# Filter treatment group only\ndf_match = df[df['treatment'] == 1]\n\n# Linear regression: 1:1 match is the omitted category\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_match).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:56:21\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nratio3\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Mean donation rates by ratio\nmean_1_1 = df_match[df_match['ratio'] == 1]['gave'].mean()\nmean_2_1 = df_match[df_match['ratio'] == 2]['gave'].mean()\nmean_3_1 = df_match[df_match['ratio'] == 3]['gave'].mean()\n\ndiff_2v1 = mean_2_1 - mean_1_1\ndiff_3v2 = mean_3_1 - mean_2_1\n\nprint(\"Response rate (2:1 - 1:1):\", round(diff_2v1, 4))\nprint(\"Response rate (3:1 - 2:1):\", round(diff_3v2, 4))\n\nResponse rate (2:1 - 1:1): 0.0019\nResponse rate (3:1 - 2:1): 0.0001\n\n\n\n# Keep only donors\ndonated = df[df['gave'] == 1]\n\n# T-test: compare mean donation amounts\namount_treatment = donated[donated['treatment'] == 1]['amount']\namount_control = donated[donated['treatment'] == 0]['amount']\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Regression: amount ~ treatment\nimport statsmodels.formula.api as smf\nmodel_amount = smf.ols(\"amount ~ treatment\", data=donated).fit()\nmodel_amount.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n22:07:00\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\namount_control = df[df['treatment'] == 0]['amount']\namount_treatment = df[df['treatment'] == 1]['amount']\nttest_amount = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nttest_amount, model_amount.summary()\n\n(TtestResult(statistic=np.float64(1.9182618934467577), pvalue=np.float64(0.05508566528918335), df=np.float64(36216.05660774625)),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     3.461\n Date:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0628\n Time:                        22:09:56   Log-Likelihood:            -1.7946e+05\n No. Observations:               50083   AIC:                         3.589e+05\n Df Residuals:                   50081   BIC:                         3.589e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.8133      0.067     12.063      0.000       0.681       0.945\n treatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n ==============================================================================\n Omnibus:                    96861.113   Durbin-Watson:                   2.008\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\n Skew:                          15.297   Prob(JB):                         0.00\n Kurtosis:                     341.269   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\ndf_positive = df[df['gave'] == 1]\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\nmodel_conditional.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n22:15:55\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport matplotlib.pyplot as plt\n\ndonated_control = df[(df['gave'] == 1) & (df['treatment'] == 0)]['amount']\ndonated_treatment = df[(df['gave'] == 1) & (df['treatment'] == 1)]['amount']\n\nmean_control = donated_control.mean()\nmean_treatment = donated_treatment.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=20, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treatment, bins=20, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].text(mean_treatment + 1, axes[1].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_treatment:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn_draws = 10000\nnp.random.seed(42)\n\n# Simulate\ncontrol_draws = np.random.binomial(1, p_control, n_draws)\ntreatment_draws = np.random.binomial(1, p_treatment, n_draws)\n\n# Difference vector\ndifferences = treatment_draws - control_draws\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\ntrue_diff = p_treatment - p_control\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. of Differences', color='orange')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers Simulation: Convergence to True Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Difference in Donation Probability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 固定随机种子以保证结果可复现\nnp.random.seed(42)\n\n# 设置参数\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# 创建子图容器\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# 对每个样本量进行模拟\nfor idx, n in enumerate(sample_sizes):\n    avg_diffs = []  # 储存每次实验的均值差\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n    # 绘图\n    axes[idx].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[idx].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[idx].set_title(f\"Sample Size = {n}\")\n    axes[idx].set_xlabel(\"Avg. Difference (Treatment - Control)\")\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].legend()\n\n# 总标题和布局\nplt.suptitle(\"Central Limit Theorem Simulation: Sampling Distributions of Avg. Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Wendy",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\nbalance_vars = {\n    'Months Since Last Donation': 'mrm2',\n    'Highest Previous Contribution': 'hpa',\n    'Number of Prior Donations': 'freq'\n}\n\n# T-tests and OLS regressions\nttest_results = {}\nols_results = {}\n\nfor label, var in balance_vars.items():\n    control_vals = df[df['treatment'] == 0][var].dropna()\n    treatment_vals = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = stats.ttest_ind(control_vals, treatment_vals, equal_var=False)\n    ttest_results[label] = {'t_stat': t_stat, 'p_val': p_val}\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n    ols_results[label] = {'coef': coef, 'p_val': pval}\n\n# Format for display\nbalance_summary = pd.DataFrame({\n    'Variable': list(balance_vars.keys()),\n    'T-test p-value': [ttest_results[v]['p_val'] for v in balance_vars],\n    'OLS coef on treatment': [ols_results[v]['coef'] for v in balance_vars],\n    'OLS p-value': [ols_results[v]['p_val'] for v in balance_vars]\n})\n\nbalance_summary\n\n\n\n\n\n\n\n\nVariable\nT-test p-value\nOLS coef on treatment\nOLS p-value\n\n\n\n\n0\nMonths Since Last Donation\n0.904855\n0.013686\n0.904886\n\n\n1\nHighest Previous Contribution\n0.331840\n0.637075\n0.345099\n\n\n2\nNumber of Prior Donations\n0.911740\n-0.011979\n0.911702\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = df.groupby('treatment')['gave'].mean()\ndonation_rates.index = ['Control', 'Treatment']\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(donation_rates.index, donation_rates.values, width=0.5)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates by Group')\n\n# Add percentage labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, height + 0.01,\n             f'{height:.2%}', ha='center', va='bottom')\n\nplt.ylim(0, max(donation_rates.values) + 0.05)\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# T-test\ncontrol_group = df[df['treatment'] == 0]['gave']\ntreatment_group = df[df['treatment'] == 1]['gave']\nt_stat, p_val = ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\nreg_summary = model.summary()\n\nt_stat, p_val, reg_summary\n\n(np.float64(3.2094621908279835),\n np.float64(0.0013309823450914173),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        15:47:55   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nimport statsmodels.api as sm\n\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment'])).fit()\n\n# 计算平均边际效应\nmarginal_effects = probit_model.get_margeff()\nmarginal_effects.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Only look at treatment group\ndf_match = df[df['treatment'] == 1]\n\n# Define match groups\ngave_1_1 = df_match[df_match['ratio'] == 1]['gave']\ngave_2_1 = df_match[df_match['ratio'] == 2]['gave']\ngave_3_1 = df_match[df_match['ratio'] == 3]['gave']\n\n# T-tests\nttest_2v1 = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 p-value:\", round(ttest_2v1.pvalue, 4))\nprint(\"3:1 vs 1:1 p-value:\", round(ttest_3v1.pvalue, 4))\n\n2:1 vs 1:1 p-value: 0.3345\n3:1 vs 1:1 p-value: 0.3101\n\n\n\nimport statsmodels.formula.api as smf\n\n# Filter treatment group only\ndf_match = df[df['treatment'] == 1]\n\n# Linear regression: 1:1 match is the omitted category\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_match).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:56:21\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nratio3\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Mean donation rates by ratio\nmean_1_1 = df_match[df_match['ratio'] == 1]['gave'].mean()\nmean_2_1 = df_match[df_match['ratio'] == 2]['gave'].mean()\nmean_3_1 = df_match[df_match['ratio'] == 3]['gave'].mean()\n\ndiff_2v1 = mean_2_1 - mean_1_1\ndiff_3v2 = mean_3_1 - mean_2_1\n\nprint(\"Response rate (2:1 - 1:1):\", round(diff_2v1, 4))\nprint(\"Response rate (3:1 - 2:1):\", round(diff_3v2, 4))\n\nResponse rate (2:1 - 1:1): 0.0019\nResponse rate (3:1 - 2:1): 0.0001\n\n\n\n# Keep only donors\ndonated = df[df['gave'] == 1]\n\n# T-test: compare mean donation amounts\namount_treatment = donated[donated['treatment'] == 1]['amount']\namount_control = donated[donated['treatment'] == 0]['amount']\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Regression: amount ~ treatment\nimport statsmodels.formula.api as smf\nmodel_amount = smf.ols(\"amount ~ treatment\", data=donated).fit()\nmodel_amount.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n22:07:00\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\namount_control = df[df['treatment'] == 0]['amount']\namount_treatment = df[df['treatment'] == 1]['amount']\nttest_amount = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nttest_amount, model_amount.summary()\n\n(TtestResult(statistic=np.float64(1.9182618934467577), pvalue=np.float64(0.05508566528918335), df=np.float64(36216.05660774625)),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     3.461\n Date:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0628\n Time:                        22:09:56   Log-Likelihood:            -1.7946e+05\n No. Observations:               50083   AIC:                         3.589e+05\n Df Residuals:                   50081   BIC:                         3.589e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.8133      0.067     12.063      0.000       0.681       0.945\n treatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n ==============================================================================\n Omnibus:                    96861.113   Durbin-Watson:                   2.008\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\n Skew:                          15.297   Prob(JB):                         0.00\n Kurtosis:                     341.269   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\ndf_positive = df[df['gave'] == 1]\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\nmodel_conditional.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nMon, 21 Apr 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n22:15:55\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport matplotlib.pyplot as plt\n\ndonated_control = df[(df['gave'] == 1) & (df['treatment'] == 0)]['amount']\ndonated_treatment = df[(df['gave'] == 1) & (df['treatment'] == 1)]['amount']\n\nmean_control = donated_control.mean()\nmean_treatment = donated_treatment.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=20, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treatment, bins=20, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].text(mean_treatment + 1, axes[1].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_treatment:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn_draws = 10000\nnp.random.seed(42)\n\n# Simulate\ncontrol_draws = np.random.binomial(1, p_control, n_draws)\ntreatment_draws = np.random.binomial(1, p_treatment, n_draws)\n\n# Difference vector\ndifferences = treatment_draws - control_draws\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\ntrue_diff = p_treatment - p_control\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. of Differences', color='orange')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers Simulation: Convergence to True Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Difference in Donation Probability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 固定随机种子以保证结果可复现\nnp.random.seed(42)\n\n# 设置参数\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# 创建子图容器\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# 对每个样本量进行模拟\nfor idx, n in enumerate(sample_sizes):\n    avg_diffs = []  # 储存每次实验的均值差\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n    # 绘图\n    axes[idx].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[idx].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[idx].set_title(f\"Sample Size = {n}\")\n    axes[idx].set_xlabel(\"Avg. Difference (Treatment - Control)\")\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].legend()\n\n# 总标题和布局\nplt.suptitle(\"Central Limit Theorem Simulation: Sampling Distributions of Avg. Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#abstract",
    "href": "projects/project1/hw1_questions.html#abstract",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Abstract",
    "text": "Abstract\nThis project seeks to replicate and interpret results from Karlan and List’s (2007) influential field experiment, which examined whether matching donations increase charitable giving. Over 50,000 prior donors were randomly assigned to receive direct mail with either no matching offer (control) or one of three matching grant treatments: $1:$1, $2:$1, or $3:$1.\nThe goal of this replication is to reproduce the study’s main empirical findings using the original dataset, explore the impact of different match ratios on both response rates and donation amounts, and provide insights relevant for fundraising practitioners."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#do-higher-matching-ratios-boost-giving",
    "href": "projects/project1/hw1_questions.html#do-higher-matching-ratios-boost-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Do Higher Matching Ratios Boost Giving?",
    "text": "Do Higher Matching Ratios Boost Giving?\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe use a series of t-tests to compare whether people are more likely to donate under higher match ratios ($2:$1 or $3:$1) compared to the standard $1:$1 match ratio. This allows us to test whether larger matching incentives increase giving behavior.\n\nfrom scipy.stats import ttest_ind\n\n# Only include treatment group\ndf_match = df[df['treatment'] == 1]\n\n# Subgroup donation outcomes\ngave_1_1 = df_match[df_match['ratio'] == 1]['gave']\ngave_2_1 = df_match[df_match['ratio'] == 2]['gave']\ngave_3_1 = df_match[df_match['ratio'] == 3]['gave']\n\n# Compute means\nmean_1_1, mean_2_1, mean_3_1 = gave_1_1.mean(), gave_2_1.mean(), gave_3_1.mean()\n\n# T-tests: compare to 1:1\nttest_2v1 = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\n\n# Output results\nprint(f\"Mean donation rates — 1:1: {mean_1_1:.4%}, 2:1: {mean_2_1:.4%}, 3:1: {mean_3_1:.4%}\")\nprint(\"2:1 vs 1:1 p-value:\", round(ttest_2v1.pvalue, 4))\nprint(\"3:1 vs 1:1 p-value:\", round(ttest_3v1.pvalue, 4))\n\nMean donation rates — 1:1: 2.0749%, 2:1: 2.2633%, 3:1: 2.2733%\n2:1 vs 1:1 p-value: 0.3345\n3:1 vs 1:1 p-value: 0.3101\n\n\nDespite slightly higher average donation rates in the 2:1 and 3:1 groups, neither difference is statistically significant. In other words, larger match offers did not significantly outperform the $1:$1 offer.\nIn essence, the act of announcing a match — any match — appears to be the key driver of increased giving. Donors respond to the presence of a match, not the generosity of its ratio. Once the match “feels impactful enough,” increasing the multiplier adds little marginal psychological value. This confirms the authors’ interpretation in the paper: “Larger match ratios had no additional impact.” (Page 8, Karlan & List 2007)"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#regression-match-ratio-effects-on-giving",
    "href": "projects/project1/hw1_questions.html#regression-match-ratio-effects-on-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Match Ratio Effects on Giving",
    "text": "Regression: Match Ratio Effects on Giving\nTo further assess whether larger match ratios increase donation behavior, we estimate a regression model where we compare 2:1 and 3:1 match rates to the baseline 1:1 match rate.\n\nimport statsmodels.formula.api as smf\n\n# Filter treatment group only\ndf_match = df[df['treatment'] == 1]\n\n# Estimate linear model: 1:1 match is omitted category\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_match).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n19:52:14\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nratio3\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThis regression estimates how larger matching ratios affect the probability of donating.\nThe intercept (0.0207) represents the average donation rate for the 1:1 match group — about 2.1%. The coefficient on ratio2 (2:1 match) is +0.0019, and the coefficient on ratio3 (3:1 match) is +0.0020. However, both of these coefficients are statistically insignificant (p-values &gt; 0.3), meaning that we cannot rule out that they are due to random chance.\nThese results reinforce what we observed in the t-tests and what Karlan & List (2007) emphasize in their paper: “Larger match ratios had no additional impact.” (Page 8)\nPeople are clearly responsive to the existence of a matching offer, but not its size. The match seems to function more like a binary psychological cue — a “yes/no” motivator — rather than a finely calibrated economic incentive.\nIn practice, this means that offering a 2:1 or 3:1 match does not significantly improve participation beyond what a 1:1 match already achieves. For fundraisers, this suggests that costlier match ratios may not provide additional return in response rate.\n\nResponse Rate Differences by Match Ratio\nWe now compute the response rate difference (i.e., difference in donation probability) between the match ratios using two methods: 1. Directly from the data 2. Using the fitted coefficients from the regression\n\nMethod 1: Directly from the data\n\n# Mean donation rates by ratio\nmean_1_1 = df_match[df_match['ratio'] == 1]['gave'].mean()\nmean_2_1 = df_match[df_match['ratio'] == 2]['gave'].mean()\nmean_3_1 = df_match[df_match['ratio'] == 3]['gave'].mean()\n\ndiff_2v1 = mean_2_1 - mean_1_1\ndiff_3v2 = mean_3_1 - mean_2_1\n\nprint(\"Response rate (2:1 - 1:1):\", round(diff_2v1, 4))\nprint(\"Response rate (3:1 - 2:1):\", round(diff_3v2, 4))\n\nResponse rate (2:1 - 1:1): 0.0019\nResponse rate (3:1 - 2:1): 0.0001\n\n\n\n\nMethod 2: Using regression coefficients\nRecall from the earlier regression:\nIntercept (1:1): 0.0207\nratio2 coefficient: +0.0019\nratio3 coefficient: +0.0020\nSo:\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0020 - 0.0019 = 0.0001\nBoth approaches yield nearly identical results:\nMoving from 1:1 to 2:1 increases the response rate by only 0.19–0.20 percentage points\nMoving from 2:1 to 3:1 increases the response rate by just 0.01 percentage points\nThese differences are statistically insignificant and extremely small. This reinforces our earlier conclusion that larger match ratios do not meaningfully increase participation. The psychological presence of a match seems to matter more than its actual size."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#size-of-charitable-contribution",
    "href": "projects/project1/hw1_questions.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, we analyze whether the presence of a matching offer influences the size of the charitable donation, conditional on a donation being made.\nWe compare donation amounts between treatment and control groups using both a t-test and a bivariate linear regression.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Split donation amount by treatment status\namount_control = df[df['treatment'] == 0]['amount']\namount_treatment = df[df['treatment'] == 1]['amount']\n\n# T-test for amount differences\nttest_amount = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Linear regression: donation amount ~ treatment\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nttest_amount, model_amount.summary()\n\n(TtestResult(statistic=np.float64(1.9182618934467577), pvalue=np.float64(0.05508566528918335), df=np.float64(36216.05660774625)),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     3.461\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\n Time:                        19:52:14   Log-Likelihood:            -1.7946e+05\n No. Observations:               50083   AIC:                         3.589e+05\n Df Residuals:                   50081   BIC:                         3.589e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.8133      0.067     12.063      0.000       0.681       0.945\n treatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n ==============================================================================\n Omnibus:                    96861.113   Durbin-Watson:                   2.008\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\n Skew:                          15.297   Prob(JB):                         0.00\n Kurtosis:                     341.269   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\nThe t-test yields a p-value of approximately 0.055, just above the standard 0.05 significance threshold. The regression model estimates that individuals in the treatment group give on average $0.15 more, with a p-value of 0.063. Both results suggest a positive but marginally insignificant effect.\nThese results provide an important nuance: While the matching offer increases the likelihood of giving, its effect on how much people give is less certain.\nSome individuals may be nudged into donating, but the size of the gift remains unaffected — or possibly even lower due to anchoring or tokenism (giving just enough to meet a perceived match).\nThis distinction underscores an important insight:\nParticipation and generosity may be driven by different behavioral mechanisms. Matching incentives are effective in getting people to act, but don’t necessarily increase the amount they give once they decide to give. From a fundraising perspective, this tells us that match incentives work best for increasing reach, not necessarily for maximizing revenue per donor."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#conditional-on-donating-did-people-give-more",
    "href": "projects/project1/hw1_questions.html#conditional-on-donating-did-people-give-more",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conditional on Donating: Did People Give More?",
    "text": "Conditional on Donating: Did People Give More?\nTo investigate whether the matching offer influences how much people give, we now restrict our analysis to only those individuals who made a donation. This allows us to analyze donation amounts conditional on giving.\n\ndf_positive = df[df['gave'] == 1]\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\nmodel_conditional.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n19:52:14\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThis regression estimates how the presence of a matching offer affects the size of the donation, among those who chose to give. The intercept (45.54) represents the average donation in the control group. The coefficient on treatment (-1.67) suggests that treated individuals gave slightly less on average — about $1.67 — but this effect is not statistically significant (p = 0.561).\nDoes This Have a Causal Interpretation?\nNo — and caution is warranted. While the original treatment assignment was randomized, this regression is conditional on donation behavior, which is a post-treatment variable. By analyzing only those who gave, we introduce selection bias — the two groups (treatment vs. control) are no longer directly comparable.\nMatching offers increase the probability of giving, as we’ve shown earlier. But among those who give, there is no significant difference in how much they donate. This suggests that matching incentives motivate action, but may not influence the donation amount once the decision to give is made. From a behavioral standpoint, this distinction is critical:\nParticipation and generosity are driven by different psychological forces. Matching works well to prompt giving — but not necessarily larger gifts.\nIf the goal is to maximize the number of donors, matching offers are effective. If the goal is to increase the average gift size, you may need a different strategy — such as social comparison, goal framing, or tiered benefits.\n\nDistribution of Donation Amounts (Donors Only)\nWe now visualize the distribution of donation amounts only among those who gave, separately for the treatment and control groups.\n\nimport matplotlib.pyplot as plt\n\ndonated_control = df[(df['gave'] == 1) & (df['treatment'] == 0)]['amount']\ndonated_treatment = df[(df['gave'] == 1) & (df['treatment'] == 1)]['amount']\n\nmean_control = donated_control.mean()\nmean_treatment = donated_treatment.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=20, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treatment, bins=20, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].text(mean_treatment + 1, axes[1].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_treatment:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that the distribution of donation amounts is right-skewed in both groups — many people donate small amounts, while a few give very large gifts.\nThe mean donation is slightly higher in the control group ($45.54) than in the treatment group ($43.87) However, as shown in the regression earlier, this difference is not statistically significant. The presence of a matching gift motivates more people to give, but does not clearly increase gift size among those who do donate."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/index.html#introduction",
    "href": "projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/index.html#abstract",
    "href": "projects/project1/index.html#abstract",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Abstract",
    "text": "Abstract\nThis project seeks to replicate and interpret results from Karlan and List’s (2007) influential field experiment, which examined whether matching donations increase charitable giving. Over 50,000 prior donors were randomly assigned to receive direct mail with either no matching offer (control) or one of three matching grant treatments: $1:$1, $2:$1, or $3:$1.\nThe goal of this replication is to reproduce the study’s main empirical findings using the original dataset, explore the impact of different match ratios on both response rates and donation amounts, and provide insights relevant for fundraising practitioners."
  },
  {
    "objectID": "projects/project1/index.html#data",
    "href": "projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe use the dataset made public by the authors, consisting of 50,083 prior donors who were randomly assigned into different treatment groups. The dataset contains variables indicating treatment status, donation behavior, suggested amounts, demographics, and political/geographic characteristics.\n\n\nSample Overview\n\nObservations: 50,083\nTreatments:\n\nControl\nMatching grants: $1:$1, $2:$1, $3:$1\n\nKey Outcomes:\n\ngave: whether the donor gave anything\namount: donation amount (if any)\n\nExample Variables:\n\nsize: suggested donation size (Unstated, $50, $100, etc.)\nredcty: lives in a red county\nhomestate: same state as charity ### Load and Preview the Data\n\n\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe test three pre-treatment variables: - Months since last donation (mrm2) - Highest previous contribution (hpa) - Number of prior donations (freq)\nThese variables should be similar across treatment and control groups if randomization was successful. We perform both t-tests and linear regressions for robustness. The table below summarizes the results.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\nbalance_vars = {\n    'Months Since Last Donation': 'mrm2',\n    'Highest Previous Contribution': 'hpa',\n    'Number of Prior Donations': 'freq'\n}\n\n# T-tests and OLS regressions\nttest_results = {}\nols_results = {}\n\nfor label, var in balance_vars.items():\n    control_vals = df[df['treatment'] == 0][var].dropna()\n    treatment_vals = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = stats.ttest_ind(control_vals, treatment_vals, equal_var=False)\n    ttest_results[label] = {'t_stat': t_stat, 'p_val': p_val}\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n    ols_results[label] = {'coef': coef, 'p_val': pval}\n\n# Format for display\nbalance_summary = pd.DataFrame({\n    'Variable': list(balance_vars.keys()),\n    'T-test p-value': [ttest_results[v]['p_val'] for v in balance_vars],\n    'OLS coef on treatment': [ols_results[v]['coef'] for v in balance_vars],\n    'OLS p-value': [ols_results[v]['p_val'] for v in balance_vars]\n})\n\nbalance_summary\n\n\n\n\n\n\n\n\nVariable\nT-test p-value\nOLS coef on treatment\nOLS p-value\n\n\n\n\n0\nMonths Since Last Donation\n0.904855\n0.013686\n0.904886\n\n\n1\nHighest Previous Contribution\n0.331840\n0.637075\n0.345099\n\n\n2\nNumber of Prior Donations\n0.911740\n-0.011979\n0.911702\n\n\n\n\n\n\n\nThe balance test confirms that randomization worked as intended. Across the three pre-treatment variables — months since last donation, highest previous contribution, and number of prior donations — none show statistically significant differences between treatment and control groups at the 5% level. ﻿ This supports the internal validity of the experiment: any observed effects in giving behavior are unlikely to be due to baseline differences."
  },
  {
    "objectID": "projects/project1/index.html#experimental-results",
    "href": "projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nMatching Donations Increase Giving Probability\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe compare the proportion of individuals who donated between the treatment group and the control group. This helps us understand whether the announcement of a matching gift increases the likelihood of donating.\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates by group\ndonation_rates = df.groupby('treatment')['gave'].mean()\ndonation_rates.index = ['Control', 'Treatment']\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(6, 4))\nbars = ax.bar(donation_rates.index, donation_rates.values, width=0.5, color=['#4C72B0', '#55A868'])\n\n# Add percentage labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.002,\n            f'{height:.2%}', ha='center', va='bottom', fontsize=10)\n\n# Formatting\nax.set_ylabel('Proportion Donated')\nax.set_title('Effect of Matching Gift on Donation Rate')\nax.set_ylim(0, max(donation_rates.values) + 0.03)\nax.grid(axis='y', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart shows that individuals in the treatment group donated at a rate of 2.20%, compared to 1.79% in the control group — an increase of approximately 23% in relative terms. This result suggests that the announcement of a matching gift offer increases donor engagement, likely because donors feel their contribution has greater impact.\nThis initial analysis replicates the paper’s key finding: matching incentives effectively nudge donors into action.\n\n\nStatistical Significance: T-test and Linear Regression\nWe test whether the observed difference in donation behavior is statistically significant, using both a t-test and a bivariate regression.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\ntreatment = df[df[\"treatment\"] == 1][\"gave\"]\nt_stat, p_val = ttest_ind(treatment, control, equal_var=False)\nprint(f\"\"\"\nT-Test Summary\n\n- T-statistic**: {t_stat:.4f}\n- P-value**: {p_val:.4f}\n\"\"\")\n\n\nT-Test Summary\n\n- T-statistic**: 3.2095\n- P-value**: 0.0013\n\n\n\n\nimport statsmodels.formula.api as smf\n\n# Fit the model\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoef = model.params[\"treatment\"]\npval = model.pvalues[\"treatment\"]\nci_low, ci_high = model.conf_int().loc[\"treatment\"]\nbaseline = model.params[\"Intercept\"]\n\nprint(f\"\"\"\nKey Findings\n\n- Baseline Donation Rate (Control Group)**: {baseline:.2%}\n- Treatment Effect**: +{coef:.2%}\n- P-value**: {pval:.4f}\n- 95% Confidence Interval**: [{ci_low:.2%}, {ci_high:.2%}]\n\"\"\")\n\n\nKey Findings\n\n- Baseline Donation Rate (Control Group)**: 1.79%\n- Treatment Effect**: +0.42%\n- P-value**: 0.0019\n- 95% Confidence Interval**: [0.15%, 0.68%]\n\n\n\nBoth tests lead to the same conclusion:\nThe t-test indicates a statistically significant difference in mean donation rates between treatment and control (p-value &lt; 0.01). The regression coefficient on treatment is +0.0042, indicating that individuals in the treatment group were 0.42 percentage points more likely to donate, which is about a 23% increase over the control group mean of 1.79%. This finding is statistically significant (p = 0.002), and replicates the original result in Table 2a Panel A of Karlan and List (2007).\nBehavioral Insight When individuals are told that their contribution will be matched, they perceive their action as more impactful. This increases their willingness to give, even if the absolute probability of donation remains low. Matching gifts operate not only as a financial lever but also as a psychological motivator, increasing perceived effectiveness.\n\n\nProbit Model with Marginal Effects\nWhile the probit coefficient on treatment was 0.087, that number cannot be directly interpreted as a change in probability. So, we compute the marginal effect of treatment.\n\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment'])).fit()\nmarginal_effects = probit_model.get_margeff()\nmarginal_effects.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\nThe marginal effect of treatment is approximately 0.004, which means that receiving a matching offer increased the probability of donation by 0.4 percentage points — exactly what the original paper reports using a linear probability model. In this way, the Probit model gives us both a robust theoretical approach and a practically interpretable result."
  },
  {
    "objectID": "projects/project1/index.html#do-higher-matching-ratios-boost-giving",
    "href": "projects/project1/index.html#do-higher-matching-ratios-boost-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Do Higher Matching Ratios Boost Giving?",
    "text": "Do Higher Matching Ratios Boost Giving?\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe use a series of t-tests to compare whether people are more likely to donate under higher match ratios ($2:$1 or $3:$1) compared to the standard $1:$1 match ratio. This allows us to test whether larger matching incentives increase giving behavior.\n\nfrom scipy.stats import ttest_ind\n\n# Only include treatment group\ndf_match = df[df['treatment'] == 1]\n\n# Subgroup donation outcomes\ngave_1_1 = df_match[df_match['ratio'] == 1]['gave']\ngave_2_1 = df_match[df_match['ratio'] == 2]['gave']\ngave_3_1 = df_match[df_match['ratio'] == 3]['gave']\n\n# Compute means\nmean_1_1, mean_2_1, mean_3_1 = gave_1_1.mean(), gave_2_1.mean(), gave_3_1.mean()\n\n# T-tests: compare to 1:1\nttest_2v1 = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\n\n# Output results\nprint(f\"Mean donation rates — 1:1: {mean_1_1:.4%}, 2:1: {mean_2_1:.4%}, 3:1: {mean_3_1:.4%}\")\nprint(\"2:1 vs 1:1 p-value:\", round(ttest_2v1.pvalue, 4))\nprint(\"3:1 vs 1:1 p-value:\", round(ttest_3v1.pvalue, 4))\n\nMean donation rates — 1:1: 2.0749%, 2:1: 2.2633%, 3:1: 2.2733%\n2:1 vs 1:1 p-value: 0.3345\n3:1 vs 1:1 p-value: 0.3101\n\n\nDespite slightly higher average donation rates in the 2:1 and 3:1 groups, neither difference is statistically significant. In other words, larger match offers did not significantly outperform the $1:$1 offer.\nIn essence, the act of announcing a match — any match — appears to be the key driver of increased giving. Donors respond to the presence of a match, not the generosity of its ratio. Once the match “feels impactful enough,” increasing the multiplier adds little marginal psychological value. This confirms the authors’ interpretation in the paper: “Larger match ratios had no additional impact.” (Page 8, Karlan & List 2007)"
  },
  {
    "objectID": "projects/project1/index.html#regression-match-ratio-effects-on-giving",
    "href": "projects/project1/index.html#regression-match-ratio-effects-on-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Match Ratio Effects on Giving",
    "text": "Regression: Match Ratio Effects on Giving\nTo further assess whether larger match ratios increase donation behavior, we estimate a regression model where we compare 2:1 and 3:1 match rates to the baseline 1:1 match rate.\n\nimport statsmodels.formula.api as smf\n\ndf_match = df[df[\"treatment\"] == 1]\n\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_match).fit()\n\ncoef = model.params\npval = model.pvalues\nci = model.conf_int()\n\nprint(f\"\"\"\nKey Results\n\n- Baseline Donation Rate (1:1 match): {coef['Intercept']:.2%}\n- 2:1 Match Effect (ratio2): {coef['ratio2']:.2%} (p = {pval['ratio2']:.3f}, 95% CI: [{ci.loc['ratio2', 0]:.2%}, {ci.loc['ratio2', 1]:.2%}])\n- 3:1 Match Effect (ratio3): {coef['ratio3']:.2%} (p = {pval['ratio3']:.3f}, 95% CI: [{ci.loc['ratio3', 0]:.2%}, {ci.loc['ratio3', 1]:.2%}])\n\"\"\")\n\n\nKey Results\n\n- Baseline Donation Rate (1:1 match): 2.07%\n- 2:1 Match Effect (ratio2): 0.19% (p = 0.338, 95% CI: [-0.20%, 0.57%])\n- 3:1 Match Effect (ratio3): 0.20% (p = 0.313, 95% CI: [-0.19%, 0.58%])\n\n\n\nThis regression estimates how larger matching ratios affect the probability of donating.\nThe intercept (0.0207) represents the average donation rate for the 1:1 match group — about 2.1%. The coefficient on ratio2 (2:1 match) is +0.0019, and the coefficient on ratio3 (3:1 match) is +0.0020. However, both of these coefficients are statistically insignificant (p-values &gt; 0.3), meaning that we cannot rule out that they are due to random chance.\nThese results reinforce what we observed in the t-tests and what Karlan & List (2007) emphasize in their paper: “Larger match ratios had no additional impact.” (Page 8)\nPeople are clearly responsive to the existence of a matching offer, but not its size. The match seems to function more like a binary psychological cue — a “yes/no” motivator — rather than a finely calibrated economic incentive.\nIn practice, this means that offering a 2:1 or 3:1 match does not significantly improve participation beyond what a 1:1 match already achieves. For fundraisers, this suggests that costlier match ratios may not provide additional return in response rate.\n\nResponse Rate Differences by Match Ratio\nWe now compute the response rate difference (i.e., difference in donation probability) between the match ratios using two methods: 1. Directly from the data 2. Using the fitted coefficients from the regression\n\nMethod 1: Directly from the data\n\n# Mean donation rates by ratio\nmean_1_1 = df_match[df_match['ratio'] == 1]['gave'].mean()\nmean_2_1 = df_match[df_match['ratio'] == 2]['gave'].mean()\nmean_3_1 = df_match[df_match['ratio'] == 3]['gave'].mean()\n\ndiff_2v1 = mean_2_1 - mean_1_1\ndiff_3v2 = mean_3_1 - mean_2_1\n\nprint(\"Response rate (2:1 - 1:1):\", round(diff_2v1, 4))\nprint(\"Response rate (3:1 - 2:1):\", round(diff_3v2, 4))\n\nResponse rate (2:1 - 1:1): 0.0019\nResponse rate (3:1 - 2:1): 0.0001\n\n\n\n\nMethod 2: Using regression coefficients\nRecall from the earlier regression:\nIntercept (1:1): 0.0207\nratio2 coefficient: +0.0019\nratio3 coefficient: +0.0020\nSo:\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0020 - 0.0019 = 0.0001\nBoth approaches yield nearly identical results:\nMoving from 1:1 to 2:1 increases the response rate by only 0.19–0.20 percentage points\nMoving from 2:1 to 3:1 increases the response rate by just 0.01 percentage points\nThese differences are statistically insignificant and extremely small. This reinforces our earlier conclusion that larger match ratios do not meaningfully increase participation. The psychological presence of a match seems to matter more than its actual size."
  },
  {
    "objectID": "projects/project1/index.html#size-of-charitable-contribution",
    "href": "projects/project1/index.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, we analyze whether the presence of a matching offer influences the size of the charitable donation, conditional on a donation being made.\nWe compare donation amounts between treatment and control groups using both a t-test and a bivariate linear regression.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Separate groups\namount_control = df[df['treatment'] == 0]['amount']\namount_treatment = df[df['treatment'] == 1]['amount']\n\n# T-test\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Regression\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# Extract results\ncoef = model_amount.params\npvals = model_amount.pvalues\nci = model_amount.conf_int()\nprint(f\"\"\"\nT-Test Result\n\n- T-statistic**: {t_stat:.4f}\n- P-value**: {p_val:.4f}\n\nRegression Summary\n\n- Baseline Amount (Control Group)**: ${coef['Intercept']:.2f}\n- Treatment Effect**: +${coef['treatment']:.2f}\n- P-value: {pvals['treatment']:.3f}\n- 95% CI: [${ci.loc['treatment', 0]:.2f}, ${ci.loc['treatment', 1]:.2f}]\n\"\"\")\n\n\nT-Test Result\n\n- T-statistic**: 1.9183\n- P-value**: 0.0551\n\nRegression Summary\n\n- Baseline Amount (Control Group)**: $0.81\n- Treatment Effect**: +$0.15\n- P-value: 0.063\n- 95% CI: [$-0.01, $0.32]\n\n\n\nThe t-test yields a p-value of approximately 0.055, just above the standard 0.05 significance threshold. The regression model estimates that individuals in the treatment group give on average $0.15 more, with a p-value of 0.063. Both results suggest a positive but marginally insignificant effect.\nThese results provide an important nuance: While the matching offer increases the likelihood of giving, its effect on how much people give is less certain.\nSome individuals may be nudged into donating, but the size of the gift remains unaffected — or possibly even lower due to anchoring or tokenism (giving just enough to meet a perceived match).\nThis distinction underscores an important insight:\nParticipation and generosity may be driven by different behavioral mechanisms. Matching incentives are effective in getting people to act, but don’t necessarily increase the amount they give once they decide to give. From a fundraising perspective, this tells us that match incentives work best for increasing reach, not necessarily for maximizing revenue per donor."
  },
  {
    "objectID": "projects/project1/index.html#conditional-on-donating-did-people-give-more",
    "href": "projects/project1/index.html#conditional-on-donating-did-people-give-more",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conditional on Donating: Did People Give More?",
    "text": "Conditional on Donating: Did People Give More?\nTo investigate whether the matching offer influences how much people give, we now restrict our analysis to only those individuals who made a donation. This allows us to analyze donation amounts conditional on giving.\n\n# Subset to donors only\ndf_positive = df[df[\"gave\"] == 1]\n\n# Run regression\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\n\n# Extract results\ncoef = model_conditional.params\npval = model_conditional.pvalues\nci = model_conditional.conf_int()\nprint(f\"\"\"\nRegression Summary\n\n- Baseline Donation (Control Group)**: ${coef['Intercept']:.2f}\n- Treatment Effect**: {coef['treatment']:+.2f}\n- P-value: {pval['treatment']:.3f}\n- 95% Confidence Interval: [{ci.loc['treatment', 0]:.2f}, {ci.loc['treatment', 1]:.2f}]\n\"\"\")\n\n\nRegression Summary\n\n- Baseline Donation (Control Group)**: $45.54\n- Treatment Effect**: -1.67\n- P-value: 0.561\n- 95% Confidence Interval: [-7.30, 3.97]\n\n\n\nThis regression estimates how the presence of a matching offer affects the size of the donation, among those who chose to give. The intercept (45.54) represents the average donation in the control group. The coefficient on treatment (-1.67) suggests that treated individuals gave slightly less on average — about $1.67 — but this effect is not statistically significant (p = 0.561).\nDoes This Have a Causal Interpretation?\nNo — and caution is warranted. While the original treatment assignment was randomized, this regression is conditional on donation behavior, which is a post-treatment variable. By analyzing only those who gave, we introduce selection bias — the two groups (treatment vs. control) are no longer directly comparable.\nMatching offers increase the probability of giving, as we’ve shown earlier. But among those who give, there is no significant difference in how much they donate. This suggests that matching incentives motivate action, but may not influence the donation amount once the decision to give is made. From a behavioral standpoint, this distinction is critical:\nParticipation and generosity are driven by different psychological forces. Matching works well to prompt giving — but not necessarily larger gifts.\nIf the goal is to maximize the number of donors, matching offers are effective. If the goal is to increase the average gift size, you may need a different strategy — such as social comparison, goal framing, or tiered benefits.\n\nDistribution of Donation Amounts (Donors Only)\nWe now visualize the distribution of donation amounts only among those who gave, separately for the treatment and control groups.\n\nimport matplotlib.pyplot as plt\n\ndonated_control = df[(df['gave'] == 1) & (df['treatment'] == 0)]['amount']\ndonated_treatment = df[(df['gave'] == 1) & (df['treatment'] == 1)]['amount']\n\nmean_control = donated_control.mean()\nmean_treatment = donated_treatment.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=20, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treatment, bins=20, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].text(mean_treatment + 1, axes[1].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_treatment:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that the distribution of donation amounts is right-skewed in both groups — many people donate small amounts, while a few give very large gifts.\nThe mean donation is slightly higher in the control group ($45.54) than in the treatment group ($43.87) However, as shown in the regression earlier, this difference is not statistically significant. The presence of a matching gift motivates more people to give, but does not clearly increase gift size among those who do donate."
  },
  {
    "objectID": "projects/project1/index.html#simulation-experiment",
    "href": "projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo simulate the LLN, we:\n\nDraw 10,000 Bernoulli samples from each group\nCalculate the difference in outcomes for each simulated pair\nPlot the cumulative average of the differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn_draws = 10000\nnp.random.seed(42)\n\n# Simulate\ncontrol_draws = np.random.binomial(1, p_control, n_draws)\ntreatment_draws = np.random.binomial(1, p_treatment, n_draws)\n\n# Difference vector\ndifferences = treatment_draws - control_draws\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\ntrue_diff = p_treatment - p_control\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. of Differences', color='orange')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers Simulation: Convergence to True Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Difference in Donation Probability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe simulation shows that the cumulative average of the differences converges to the true value of 0.004, which is the expected treatment effect. This is a visual and intuitive demonstration of the Law of Large Numbers: with more data, our sample statistics converge to population parameters.\nThis validates the logic behind the t-test and confirms why large samples allow for reliable estimation of small effects — just like in Karlan & List (2007), where a small increase in giving (from 1.8% to 2.2%) was detected across a large sample.\n\n\nCentral Limit Theorem\nTo visualize the Central Limit Theorem, we simulate 1,000 experiments for each of four sample sizes: 50, 200, 500, and 1000. In each simulation, we:\n\nDraw n observations from a Bernoulli(p=0.018) distribution (control group)\nDraw n from a Bernoulli(p=0.022) distribution (treatment group)\nCalculate the difference in sample means (treatment - control)\nRepeat 1,000 times and plot the histogram of these average differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor idx, n in enumerate(sample_sizes):\n    avg_diffs = [] \n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n  \n    axes[idx].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[idx].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[idx].set_title(f\"Sample Size = {n}\")\n    axes[idx].set_xlabel(\"Avg. Difference (Treatment - Control)\")\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].legend()\n\nplt.suptitle(\"Central Limit Theorem Simulation: Sampling Distributions of Avg. Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that:\nAs sample size increases, the distribution of average differences becomes tighter and more symmetric For small sample sizes (e.g., n = 50), the histogram is more spread out and zero often appears near the center For larger sample sizes (n = 500 or 1000), the distribution concentrates around the true mean difference (~0.004), and zero moves closer to the edge or tail This behavior demonstrates the Central Limit Theorem in action:\nEven though the original data (Bernoulli) is highly skewed, the sampling distribution of the mean difference becomes approximately normal, and its variance shrinks with larger n.\nThis validates why Karlan & List’s experiment was able to detect a small effect (2.2% vs 1.8%) — their large sample size ensured that sampling variation wouldn’t drown out the true effect."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html",
    "href": "projects/project1/index.qmd 21-36-38-121.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#introduction",
    "href": "projects/project1/index.qmd 21-36-38-121.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#abstract",
    "href": "projects/project1/index.qmd 21-36-38-121.html#abstract",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Abstract",
    "text": "Abstract\nThis project seeks to replicate and interpret results from Karlan and List’s (2007) influential field experiment, which examined whether matching donations increase charitable giving. Over 50,000 prior donors were randomly assigned to receive direct mail with either no matching offer (control) or one of three matching grant treatments: $1:$1, $2:$1, or $3:$1.\nThe goal of this replication is to reproduce the study’s main empirical findings using the original dataset, explore the impact of different match ratios on both response rates and donation amounts, and provide insights relevant for fundraising practitioners."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#data",
    "href": "projects/project1/index.qmd 21-36-38-121.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe use the dataset made public by the authors, consisting of 50,083 prior donors who were randomly assigned into different treatment groups. The dataset contains variables indicating treatment status, donation behavior, suggested amounts, demographics, and political/geographic characteristics.\n\n\nSample Overview\n\nObservations: 50,083\nTreatments:\n\nControl\nMatching grants: $1:$1, $2:$1, $3:$1\n\nKey Outcomes:\n\ngave: whether the donor gave anything\namount: donation amount (if any)\n\nExample Variables:\n\nsize: suggested donation size (Unstated, $50, $100, etc.)\nredcty: lives in a red county\nhomestate: same state as charity ### Load and Preview the Data\n\n\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe test three pre-treatment variables: - Months since last donation (mrm2) - Highest previous contribution (hpa) - Number of prior donations (freq)\nThese variables should be similar across treatment and control groups if randomization was successful. We perform both t-tests and linear regressions for robustness. The table below summarizes the results.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\nbalance_vars = {\n    'Months Since Last Donation': 'mrm2',\n    'Highest Previous Contribution': 'hpa',\n    'Number of Prior Donations': 'freq'\n}\n\n# T-tests and OLS regressions\nttest_results = {}\nols_results = {}\n\nfor label, var in balance_vars.items():\n    control_vals = df[df['treatment'] == 0][var].dropna()\n    treatment_vals = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = stats.ttest_ind(control_vals, treatment_vals, equal_var=False)\n    ttest_results[label] = {'t_stat': t_stat, 'p_val': p_val}\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n    ols_results[label] = {'coef': coef, 'p_val': pval}\n\n# Format for display\nbalance_summary = pd.DataFrame({\n    'Variable': list(balance_vars.keys()),\n    'T-test p-value': [ttest_results[v]['p_val'] for v in balance_vars],\n    'OLS coef on treatment': [ols_results[v]['coef'] for v in balance_vars],\n    'OLS p-value': [ols_results[v]['p_val'] for v in balance_vars]\n})\n\nbalance_summary\n\n\n\n\n\n\n\n\nVariable\nT-test p-value\nOLS coef on treatment\nOLS p-value\n\n\n\n\n0\nMonths Since Last Donation\n0.904855\n0.013686\n0.904886\n\n\n1\nHighest Previous Contribution\n0.331840\n0.637075\n0.345099\n\n\n2\nNumber of Prior Donations\n0.911740\n-0.011979\n0.911702\n\n\n\n\n\n\n\nThe balance test confirms that randomization worked as intended. Across the three pre-treatment variables — months since last donation, highest previous contribution, and number of prior donations — none show statistically significant differences between treatment and control groups at the 5% level. ﻿ This supports the internal validity of the experiment: any observed effects in giving behavior are unlikely to be due to baseline differences."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#experimental-results",
    "href": "projects/project1/index.qmd 21-36-38-121.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nMatching Donations Increase Giving Probability\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe compare the proportion of individuals who donated between the treatment group and the control group. This helps us understand whether the announcement of a matching gift increases the likelihood of donating.\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates by group\ndonation_rates = df.groupby('treatment')['gave'].mean()\ndonation_rates.index = ['Control', 'Treatment']\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(6, 4))\nbars = ax.bar(donation_rates.index, donation_rates.values, width=0.5, color=['#4C72B0', '#55A868'])\n\n# Add percentage labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.002,\n            f'{height:.2%}', ha='center', va='bottom', fontsize=10)\n\n# Formatting\nax.set_ylabel('Proportion Donated')\nax.set_title('Effect of Matching Gift on Donation Rate')\nax.set_ylim(0, max(donation_rates.values) + 0.03)\nax.grid(axis='y', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe chart shows that individuals in the treatment group donated at a rate of 2.20%, compared to 1.79% in the control group — an increase of approximately 23% in relative terms. This result suggests that the announcement of a matching gift offer increases donor engagement, likely because donors feel their contribution has greater impact.\nThis initial analysis replicates the paper’s key finding: matching incentives effectively nudge donors into action.\n\n\nStatistical Significance: T-test and Linear Regression\nWe test whether the observed difference in donation behavior is statistically significant, using both a t-test and a bivariate regression.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\ntreatment = df[df[\"treatment\"] == 1][\"gave\"]\nt_stat, p_val = ttest_ind(treatment, control, equal_var=False)\nprint(f\"\"\"\nT-Test Summary\n\n- T-statistic**: {t_stat:.4f}\n- P-value**: {p_val:.4f}\n\"\"\")\n\n\nT-Test Summary\n\n- T-statistic**: 3.2095\n- P-value**: 0.0013\n\n\n\n\nimport statsmodels.formula.api as smf\n\n# Fit the model\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoef = model.params[\"treatment\"]\npval = model.pvalues[\"treatment\"]\nci_low, ci_high = model.conf_int().loc[\"treatment\"]\nbaseline = model.params[\"Intercept\"]\n\nprint(f\"\"\"\nKey Findings\n\n- Baseline Donation Rate (Control Group)**: {baseline:.2%}\n- Treatment Effect**: +{coef:.2%}\n- P-value**: {pval:.4f}\n- 95% Confidence Interval**: [{ci_low:.2%}, {ci_high:.2%}]\n\"\"\")\n\n\nKey Findings\n\n- Baseline Donation Rate (Control Group)**: 1.79%\n- Treatment Effect**: +0.42%\n- P-value**: 0.0019\n- 95% Confidence Interval**: [0.15%, 0.68%]\n\n\n\nBoth tests lead to the same conclusion:\nThe t-test indicates a statistically significant difference in mean donation rates between treatment and control (p-value &lt; 0.01). The regression coefficient on treatment is +0.0042, indicating that individuals in the treatment group were 0.42 percentage points more likely to donate, which is about a 23% increase over the control group mean of 1.79%. This finding is statistically significant (p = 0.002), and replicates the original result in Table 2a Panel A of Karlan and List (2007).\nBehavioral Insight When individuals are told that their contribution will be matched, they perceive their action as more impactful. This increases their willingness to give, even if the absolute probability of donation remains low. Matching gifts operate not only as a financial lever but also as a psychological motivator, increasing perceived effectiveness.\n\n\nProbit Model with Marginal Effects\nWhile the probit coefficient on treatment was 0.087, that number cannot be directly interpreted as a change in probability. So, we compute the marginal effect of treatment.\n\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment'])).fit()\nmarginal_effects = probit_model.get_margeff()\nmarginal_effects.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Marginal Effects\n\n\nDep. Variable:\ngave\n\n\nMethod:\ndydx\n\n\nAt:\noverall\n\n\n\n\n\n\n\n\n\ndy/dx\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n0.0043\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\nThe marginal effect of treatment is approximately 0.004, which means that receiving a matching offer increased the probability of donation by 0.4 percentage points — exactly what the original paper reports using a linear probability model. In this way, the Probit model gives us both a robust theoretical approach and a practically interpretable result."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#do-higher-matching-ratios-boost-giving",
    "href": "projects/project1/index.qmd 21-36-38-121.html#do-higher-matching-ratios-boost-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Do Higher Matching Ratios Boost Giving?",
    "text": "Do Higher Matching Ratios Boost Giving?\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe use a series of t-tests to compare whether people are more likely to donate under higher match ratios ($2:$1 or $3:$1) compared to the standard $1:$1 match ratio. This allows us to test whether larger matching incentives increase giving behavior.\n\nfrom scipy.stats import ttest_ind\n\n# Only include treatment group\ndf_match = df[df['treatment'] == 1]\n\n# Subgroup donation outcomes\ngave_1_1 = df_match[df_match['ratio'] == 1]['gave']\ngave_2_1 = df_match[df_match['ratio'] == 2]['gave']\ngave_3_1 = df_match[df_match['ratio'] == 3]['gave']\n\n# Compute means\nmean_1_1, mean_2_1, mean_3_1 = gave_1_1.mean(), gave_2_1.mean(), gave_3_1.mean()\n\n# T-tests: compare to 1:1\nttest_2v1 = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\n\n# Output results\nprint(f\"Mean donation rates — 1:1: {mean_1_1:.4%}, 2:1: {mean_2_1:.4%}, 3:1: {mean_3_1:.4%}\")\nprint(\"2:1 vs 1:1 p-value:\", round(ttest_2v1.pvalue, 4))\nprint(\"3:1 vs 1:1 p-value:\", round(ttest_3v1.pvalue, 4))\n\nMean donation rates — 1:1: 2.0749%, 2:1: 2.2633%, 3:1: 2.2733%\n2:1 vs 1:1 p-value: 0.3345\n3:1 vs 1:1 p-value: 0.3101\n\n\nDespite slightly higher average donation rates in the 2:1 and 3:1 groups, neither difference is statistically significant. In other words, larger match offers did not significantly outperform the $1:$1 offer.\nIn essence, the act of announcing a match — any match — appears to be the key driver of increased giving. Donors respond to the presence of a match, not the generosity of its ratio. Once the match “feels impactful enough,” increasing the multiplier adds little marginal psychological value. This confirms the authors’ interpretation in the paper: “Larger match ratios had no additional impact.” (Page 8, Karlan & List 2007)"
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#regression-match-ratio-effects-on-giving",
    "href": "projects/project1/index.qmd 21-36-38-121.html#regression-match-ratio-effects-on-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Match Ratio Effects on Giving",
    "text": "Regression: Match Ratio Effects on Giving\nTo further assess whether larger match ratios increase donation behavior, we estimate a regression model where we compare 2:1 and 3:1 match rates to the baseline 1:1 match rate.\n\nimport statsmodels.formula.api as smf\n\ndf_match = df[df[\"treatment\"] == 1]\n\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_match).fit()\n\ncoef = model.params\npval = model.pvalues\nci = model.conf_int()\n\nprint(f\"\"\"\nKey Results\n\n- Baseline Donation Rate (1:1 match): {coef['Intercept']:.2%}\n- 2:1 Match Effect (ratio2): {coef['ratio2']:.2%} (p = {pval['ratio2']:.3f}, 95% CI: [{ci.loc['ratio2', 0]:.2%}, {ci.loc['ratio2', 1]:.2%}])\n- 3:1 Match Effect (ratio3): {coef['ratio3']:.2%} (p = {pval['ratio3']:.3f}, 95% CI: [{ci.loc['ratio3', 0]:.2%}, {ci.loc['ratio3', 1]:.2%}])\n\"\"\")\n\n\nKey Results\n\n- Baseline Donation Rate (1:1 match): 2.07%\n- 2:1 Match Effect (ratio2): 0.19% (p = 0.338, 95% CI: [-0.20%, 0.57%])\n- 3:1 Match Effect (ratio3): 0.20% (p = 0.313, 95% CI: [-0.19%, 0.58%])\n\n\n\nThis regression estimates how larger matching ratios affect the probability of donating.\nThe intercept (0.0207) represents the average donation rate for the 1:1 match group — about 2.1%. The coefficient on ratio2 (2:1 match) is +0.0019, and the coefficient on ratio3 (3:1 match) is +0.0020. However, both of these coefficients are statistically insignificant (p-values &gt; 0.3), meaning that we cannot rule out that they are due to random chance.\nThese results reinforce what we observed in the t-tests and what Karlan & List (2007) emphasize in their paper: “Larger match ratios had no additional impact.” (Page 8)\nPeople are clearly responsive to the existence of a matching offer, but not its size. The match seems to function more like a binary psychological cue — a “yes/no” motivator — rather than a finely calibrated economic incentive.\nIn practice, this means that offering a 2:1 or 3:1 match does not significantly improve participation beyond what a 1:1 match already achieves. For fundraisers, this suggests that costlier match ratios may not provide additional return in response rate.\n\nResponse Rate Differences by Match Ratio\nWe now compute the response rate difference (i.e., difference in donation probability) between the match ratios using two methods: 1. Directly from the data 2. Using the fitted coefficients from the regression\n\nMethod 1: Directly from the data\n\n# Mean donation rates by ratio\nmean_1_1 = df_match[df_match['ratio'] == 1]['gave'].mean()\nmean_2_1 = df_match[df_match['ratio'] == 2]['gave'].mean()\nmean_3_1 = df_match[df_match['ratio'] == 3]['gave'].mean()\n\ndiff_2v1 = mean_2_1 - mean_1_1\ndiff_3v2 = mean_3_1 - mean_2_1\n\nprint(\"Response rate (2:1 - 1:1):\", round(diff_2v1, 4))\nprint(\"Response rate (3:1 - 2:1):\", round(diff_3v2, 4))\n\nResponse rate (2:1 - 1:1): 0.0019\nResponse rate (3:1 - 2:1): 0.0001\n\n\n\n\nMethod 2: Using regression coefficients\nRecall from the earlier regression:\nIntercept (1:1): 0.0207\nratio2 coefficient: +0.0019\nratio3 coefficient: +0.0020\nSo:\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0020 - 0.0019 = 0.0001\nBoth approaches yield nearly identical results:\nMoving from 1:1 to 2:1 increases the response rate by only 0.19–0.20 percentage points\nMoving from 2:1 to 3:1 increases the response rate by just 0.01 percentage points\nThese differences are statistically insignificant and extremely small. This reinforces our earlier conclusion that larger match ratios do not meaningfully increase participation. The psychological presence of a match seems to matter more than its actual size."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#size-of-charitable-contribution",
    "href": "projects/project1/index.qmd 21-36-38-121.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, we analyze whether the presence of a matching offer influences the size of the charitable donation, conditional on a donation being made.\nWe compare donation amounts between treatment and control groups using both a t-test and a bivariate linear regression.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Separate groups\namount_control = df[df['treatment'] == 0]['amount']\namount_treatment = df[df['treatment'] == 1]['amount']\n\n# T-test\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Regression\nmodel_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# Extract results\ncoef = model_amount.params\npvals = model_amount.pvalues\nci = model_amount.conf_int()\nprint(f\"\"\"\nT-Test Result\n\n- T-statistic**: {t_stat:.4f}\n- P-value**: {p_val:.4f}\n\nRegression Summary\n\n- Baseline Amount (Control Group)**: ${coef['Intercept']:.2f}\n- Treatment Effect**: +${coef['treatment']:.2f}\n- P-value: {pvals['treatment']:.3f}\n- 95% CI: [${ci.loc['treatment', 0]:.2f}, ${ci.loc['treatment', 1]:.2f}]\n\"\"\")\n\n\nT-Test Result\n\n- T-statistic**: 1.9183\n- P-value**: 0.0551\n\nRegression Summary\n\n- Baseline Amount (Control Group)**: $0.81\n- Treatment Effect**: +$0.15\n- P-value: 0.063\n- 95% CI: [$-0.01, $0.32]\n\n\n\nThe t-test yields a p-value of approximately 0.055, just above the standard 0.05 significance threshold. The regression model estimates that individuals in the treatment group give on average $0.15 more, with a p-value of 0.063. Both results suggest a positive but marginally insignificant effect.\nThese results provide an important nuance: While the matching offer increases the likelihood of giving, its effect on how much people give is less certain.\nSome individuals may be nudged into donating, but the size of the gift remains unaffected — or possibly even lower due to anchoring or tokenism (giving just enough to meet a perceived match).\nThis distinction underscores an important insight:\nParticipation and generosity may be driven by different behavioral mechanisms. Matching incentives are effective in getting people to act, but don’t necessarily increase the amount they give once they decide to give. From a fundraising perspective, this tells us that match incentives work best for increasing reach, not necessarily for maximizing revenue per donor."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#conditional-on-donating-did-people-give-more",
    "href": "projects/project1/index.qmd 21-36-38-121.html#conditional-on-donating-did-people-give-more",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conditional on Donating: Did People Give More?",
    "text": "Conditional on Donating: Did People Give More?\nTo investigate whether the matching offer influences how much people give, we now restrict our analysis to only those individuals who made a donation. This allows us to analyze donation amounts conditional on giving.\n\n# Subset to donors only\ndf_positive = df[df[\"gave\"] == 1]\n\n# Run regression\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\n\n# Extract results\ncoef = model_conditional.params\npval = model_conditional.pvalues\nci = model_conditional.conf_int()\nprint(f\"\"\"\nRegression Summary\n\n- Baseline Donation (Control Group)**: ${coef['Intercept']:.2f}\n- Treatment Effect**: {coef['treatment']:+.2f}\n- P-value: {pval['treatment']:.3f}\n- 95% Confidence Interval: [{ci.loc['treatment', 0]:.2f}, {ci.loc['treatment', 1]:.2f}]\n\"\"\")\n\n\nRegression Summary\n\n- Baseline Donation (Control Group)**: $45.54\n- Treatment Effect**: -1.67\n- P-value: 0.561\n- 95% Confidence Interval: [-7.30, 3.97]\n\n\n\nThis regression estimates how the presence of a matching offer affects the size of the donation, among those who chose to give. The intercept (45.54) represents the average donation in the control group. The coefficient on treatment (-1.67) suggests that treated individuals gave slightly less on average — about $1.67 — but this effect is not statistically significant (p = 0.561).\nDoes This Have a Causal Interpretation?\nNo — and caution is warranted. While the original treatment assignment was randomized, this regression is conditional on donation behavior, which is a post-treatment variable. By analyzing only those who gave, we introduce selection bias — the two groups (treatment vs. control) are no longer directly comparable.\nMatching offers increase the probability of giving, as we’ve shown earlier. But among those who give, there is no significant difference in how much they donate. This suggests that matching incentives motivate action, but may not influence the donation amount once the decision to give is made. From a behavioral standpoint, this distinction is critical:\nParticipation and generosity are driven by different psychological forces. Matching works well to prompt giving — but not necessarily larger gifts.\nIf the goal is to maximize the number of donors, matching offers are effective. If the goal is to increase the average gift size, you may need a different strategy — such as social comparison, goal framing, or tiered benefits.\n\nDistribution of Donation Amounts (Donors Only)\nWe now visualize the distribution of donation amounts only among those who gave, separately for the treatment and control groups.\n\nimport matplotlib.pyplot as plt\n\ndonated_control = df[(df['gave'] == 1) & (df['treatment'] == 0)]['amount']\ndonated_treatment = df[(df['gave'] == 1) & (df['treatment'] == 1)]['amount']\n\nmean_control = donated_control.mean()\nmean_treatment = donated_treatment.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=20, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treatment, bins=20, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].text(mean_treatment + 1, axes[1].get_ylim()[1] * 0.8,\n             f'Mean: ${mean_treatment:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that the distribution of donation amounts is right-skewed in both groups — many people donate small amounts, while a few give very large gifts.\nThe mean donation is slightly higher in the control group ($45.54) than in the treatment group ($43.87) However, as shown in the regression earlier, this difference is not statistically significant. The presence of a matching gift motivates more people to give, but does not clearly increase gift size among those who do donate."
  },
  {
    "objectID": "projects/project1/index.qmd 21-36-38-121.html#simulation-experiment",
    "href": "projects/project1/index.qmd 21-36-38-121.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo simulate the LLN, we:\n\nDraw 10,000 Bernoulli samples from each group\nCalculate the difference in outcomes for each simulated pair\nPlot the cumulative average of the differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn_draws = 10000\nnp.random.seed(42)\n\n# Simulate\ncontrol_draws = np.random.binomial(1, p_control, n_draws)\ntreatment_draws = np.random.binomial(1, p_treatment, n_draws)\n\n# Difference vector\ndifferences = treatment_draws - control_draws\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\ntrue_diff = p_treatment - p_control\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. of Differences', color='orange')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers Simulation: Convergence to True Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Difference in Donation Probability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe simulation shows that the cumulative average of the differences converges to the true value of 0.004, which is the expected treatment effect. This is a visual and intuitive demonstration of the Law of Large Numbers: with more data, our sample statistics converge to population parameters.\nThis validates the logic behind the t-test and confirms why large samples allow for reliable estimation of small effects — just like in Karlan & List (2007), where a small increase in giving (from 1.8% to 2.2%) was detected across a large sample.\n\n\nCentral Limit Theorem\nTo visualize the Central Limit Theorem, we simulate 1,000 experiments for each of four sample sizes: 50, 200, 500, and 1000. In each simulation, we:\n\nDraw n observations from a Bernoulli(p=0.018) distribution (control group)\nDraw n from a Bernoulli(p=0.022) distribution (treatment group)\nCalculate the difference in sample means (treatment - control)\nRepeat 1,000 times and plot the histogram of these average differences\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor idx, n in enumerate(sample_sizes):\n    avg_diffs = [] \n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n  \n    axes[idx].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[idx].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[idx].set_title(f\"Sample Size = {n}\")\n    axes[idx].set_xlabel(\"Avg. Difference (Treatment - Control)\")\n    axes[idx].set_ylabel(\"Frequency\")\n    axes[idx].legend()\n\nplt.suptitle(\"Central Limit Theorem Simulation: Sampling Distributions of Avg. Differences\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that:\nAs sample size increases, the distribution of average differences becomes tighter and more symmetric For small sample sizes (e.g., n = 50), the histogram is more spread out and zero often appears near the center For larger sample sizes (n = 500 or 1000), the distribution concentrates around the true mean difference (~0.004), and zero moves closer to the edge or tail This behavior demonstrates the Central Limit Theorem in action:\nEven though the original data (Bernoulli) is highly skewed, the sampling distribution of the mean difference becomes approximately normal, and its variance shrinks with larger n.\nThis validates why Karlan & List’s experiment was able to detect a small effect (2.2% vs 1.8%) — their large sample size ensured that sampling variation wouldn’t drown out the true effect."
  },
  {
    "objectID": "projects/project2/hw2_questions.html",
    "href": "projects/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\ndf.info()\ndf.describe(include=\"all\")\ndf.isnull().sum()\nplt.figure(figsize=(8, 4))\nplt.hist(df['patents'], bins=30)\nplt.title('Distribution of Patents Awarded')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.histplot(data=df, x='patents', hue='iscustomer', bins=30, multiple='dodge')\nplt.title('Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.legend(title='Uses Blueprinty', labels=['No', 'Yes'])\nplt.show()\ndf.groupby('iscustomer')['patents'].mean()\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nFrom the histograms, we observe that firms using Blueprinty’s software tend to have a distribution shifted slightly to the right compared to those not using the software, suggesting higher patent counts.\nThe mean number of patents is higher among customers of Blueprinty than non-customers, which may suggest an association between using the software and patenting success. However, further modeling is required to control for other factors like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport numpy as np\n\nplt.figure(figsize=(10, 5))\n\nbins = np.arange(8, 51, 1) \n\nplt.hist(df[df['iscustomer'] == 0]['age'], bins=bins - 0.2, width=0.4, alpha=0.7, label='Not Using Blueprinty', color='skyblue')\nplt.hist(df[df['iscustomer'] == 1]['age'], bins=bins + 0.2, width=0.4, alpha=0.7, label='Using Blueprinty', color='orange')\n\nplt.title('Firm Age Distribution by Customer Status (Side-by-Side)')\nplt.xlabel('Firm Age')\nplt.ylabel('Number of Firms')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index')\nregion_counts.columns = ['Not Using', 'Using']\nregion_counts.plot(kind='bar', stacked=True, figsize=(8, 5))\nplt.title('Regional Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Proportion')\nplt.legend(title='Uses Blueprinty', loc='upper left', bbox_to_anchor=(1.02, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirms using Blueprinty software tend to be older, on average, than those not using it. Age might be a confounder and should be controlled for in any causal inference or regression analysis.\nThere are regional differences in Blueprinty software adoption, with the Northeast having a much higher proportion of customers. Region could be a significant predictor or confounder and should be included in modeling.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for the Poisson model is based on the assumption that the outcome variable \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\(\\lambda_i = \\exp(X_i \\beta)\\).\nThe probability mass function for a Poisson-distributed outcome is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations, the overall likelihood function for (n) firms is:\n\\[\nL(\\beta) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}, \\quad \\text{where } \\lambda_i = \\exp(X_i \\beta)\n\\]\nTaking the logarithm, the log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ -\\exp(X_i \\beta) + Y_i X_i \\beta - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n    \n    Parameters:\n        lmbda: array-like, predicted Poisson means (e.g. exp(X @ beta))\n        y: array-like, observed counts\n\n    Returns:\n        log-likelihood value (scalar)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\n\n\n\ny_obs_full = df['patents'].values\n\nlambda_range = np.linspace(0.1, 10, 200)\n\nlog_likelihoods_full = [poisson_loglikelihood(lmbda=np.repeat(lmb, len(y_obs_full)), y=y_obs_full)\n                        for lmb in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, log_likelihoods_full)\nplt.title('Poisson Log-Likelihood vs Lambda (Full Sample)')\nplt.xlabel('Lambda (shared across firms)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) of \\(\\lambda\\), we start from the Poisson log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean is the natural estimate of the population mean under the MLE framework.\n\nlambda_mle = df['patents'].mean()\nprint(f\"Lambda MLE (sample mean of Y): {lambda_mle:.4f}\")\n\nLambda MLE (sample mean of Y): 3.6847\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndef neg_log_likelihood(lmbda_scalar, y):\n    lmbda = np.repeat(lmbda_scalar[0], len(y))\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny_data = df['patents'].values\n\ninitial_guess = [np.mean(y_data)]\n\nresult = minimize(neg_log_likelihood, x0=initial_guess, args=(y_data,), bounds=[(0.0001, None)])\n\nlambda_mle = result.x[0]\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef neg_poisson_loglikelihood(beta, X, y):\n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n\n    linear_pred = X @ beta\n    lmbda = np.exp(linear_pred)\n    log_lik = -lmbda + y * linear_pred - gammaln(y + 1)\n    return -np.sum(log_lik)\n\n\n\n\nfrom scipy.optimize import minimize\n\nX = df[['age']].copy()\nX['age_squared'] = X['age']**2\nX['iscustomer'] = df['iscustomer']\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float)\nX = X.values\n\ny = df['patents'].values\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 1000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3269.138673\n         Iterations: 376\n         Function evaluations: 588\nEstimated beta coefficients:\n[ 0.11541318 -0.00237274  0.10391813  0.02789235 -0.07511018 -0.01228066\n -0.05693624]\n\n\nThe Poisson regression model estimated the impact of firm characteristics on patent counts. We find that firm age is positively associated with patent output, while being a Blueprinty customer is also linked to a higher expected number of patents. Regional effects are small and mixed, with most being slightly negative relative to the omitted baseline region.\n\nX = df[['age']].copy()\nX['age_squared'] = X['age'] ** 2\nX['iscustomer'] = df['iscustomer']\n\nX.insert(0, 'intercept', 1)\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float).values\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 2000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3259.214530\n         Iterations: 685\n         Function evaluations: 1054\nEstimated beta coefficients:\n[-0.32950681  0.13700644 -0.00276358  0.21449006 -0.01353866 -0.05320079\n  0.02214911  0.04299568]\n\n\n\nimport pandas as pd\nimport numdifftools as nd\n\nhessian_fun = nd.Hessian(lambda b: neg_poisson_loglikelihood(b, X, y))\nH = hessian_fun(beta_mle)\n\ncov_matrix = np.linalg.inv(H)\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\ncolumns = ['intercept', 'age', 'age_squared', 'iscustomer'] + list(pd.get_dummies(df['region'], drop_first=True).columns)\n\nresults_table = pd.DataFrame({\n    'Variable': columns,\n    'Beta': beta_mle,\n    'Std. Error': standard_errors\n})\n\nresults_table\n\n/var/folders/32/xz9pd3zx1yv2tk_19psx52z40000gn/T/ipykernel_25408/820769554.py:11: RuntimeWarning: overflow encountered in exp\n  lmbda = np.exp(linear_pred)\n\n\n\n\n\n\n\n\n\nVariable\nBeta\nStd. Error\n\n\n\n\n0\nintercept\n-0.329507\n6.520750e-02\n\n\n1\nage\n0.137006\n2.057879e-03\n\n\n2\nage_squared\n-0.002764\n1.473287e-09\n\n\n3\niscustomer\n0.214490\n3.075139e-02\n\n\n4\nNortheast\n-0.013539\n4.321331e-02\n\n\n5\nNorthwest\n-0.053201\n5.356145e-02\n\n\n6\nSouth\n0.022149\n5.239738e-02\n\n\n7\nSouthwest\n0.042996\n4.660574e-02\n\n\n\n\n\n\n\nThe inclusion of a constant term (intercept = -0.3295) ensures the model accounts for a baseline patent rate when all covariates are zero.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_check = df[['age']].copy()\nX_check['age_squared'] = X_check['age']**2\nX_check['iscustomer'] = df['iscustomer']\nX_check = pd.concat([X_check, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX_check = sm.add_constant(X_check) \nX_check = X_check.astype(float)\n\nglm_poisson = sm.GLM(df['patents'], X_check, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\n\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:28:33   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\n\nThe results are mostly consistent, with some small differences in coefficient values (possibly due to different scaling, convergence settings, or data preprocessing).\n\n\n\n\nThis analysis uses a Poisson regression model to examine the factors influencing the number of patents. The model is specified as a Generalized Linear Model (GLM) with a log link and Poisson distribution.\n\n\n\nDependent Variable: patents (number of patents)\nSample Size: 1500 observations\n\nPseudo R-squared: 0.1360 (indicates moderate explanatory power)\n\nLink Function: Log\n\nFitting Method: IRLS (Iteratively Reweighted Least Squares)\n\n\n\n\n\nBelow is the interpretation of key coefficients:\nIntercept (const = -0.5089)\nWhen all predictors are set to zero, the expected log count of patents is -0.5089. Converting from log to count scale: exp(-0.5089) ≈ 0.60 → the baseline expected number of patents is approximately 0.60.\nAge (coef = 0.1486, p &lt; 0.001)\nEach additional year of age increases the log of the expected patent count by 0.1486. On the count scale: exp(0.1486) ≈ 1.16, meaning a 16% increase in the expected number of patents per additional year of age.\nAge Squared (coef = -0.0030, p &lt; 0.001)\nThe negative coefficient indicates a concave (inverted-U) relationship between age and patents: the number of patents increases with age up to a certain point, then starts to decline.\nIs Customer (coef = 0.2076, p &lt; 0.001)\nBeing a customer increases the expected number of patents. exp(0.2076) ≈ 1.23 → customers are expected to have 23% more patents than non-customers.\nRegion Variables (Northeast, Northwest, South, Southwest)\nNone of these regional indicators are statistically significant (p-values &gt; 0.05), suggesting no strong evidence that geographic region affects patent counts in this model.\nIn summary, Age has a significant nonlinear effect on patent count. Customer status is a strong and significant positive predictor. Regional location appears not to significantly affect patent outcomes. The model captures meaningful patterns, though its overall explanatory power is moderate (Pseudo R² = 0.136)\n\n\n\nTo understand the effect of using Blueprinty’s software (represented by the iscustomer variable) on patent success, we estimate the average treatment effect (ATE). Since we are using a Poisson regression with a log link, the beta coefficient for iscustomer is not directly interpretable in units of patent count.\nInstead, we use a counterfactual prediction approach:\n\nCreate two copies of the dataset:\n\nX_0: with iscustomer = 0 for all firms (non-customers)\nX_1: with iscustomer = 1 for all firms (customers)\n\nUse the fitted model to generate predicted patent counts for each dataset:\n\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\nCompute the individual treatment effects (y_pred_1 - y_pred_0), and take their average.\n\n\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\n# Assuming X and y are your data\nX_df = pd.DataFrame(X, columns=[\n    'intercept', 'age', 'age_squared', 'iscustomer',\n    'Northeast', 'Northwest', 'South', 'Southwest'\n])\n\n# Fit the model if it's not already done\nmodel = sm.GLM(y, X_df, family=sm.families.Poisson(link=sm.families.links.log())).fit()\n\n# Now create counterfactuals\nX_0 = X_df.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X_df.copy()\nX_1['iscustomer'] = 1\n\n# Predict\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\n# Treatment effect\ntreatment_effect = y_pred_1 - y_pred_0\naverage_effect = treatment_effect.mean()\n\naverage_effect\n\n/Users/wenxinxu/Library/Python/3.9/lib/python/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n  warnings.warn(\n\n\nnp.float64(0.7927680710452972)\n\n\n\n\n\nOn average, being a Blueprinty customer is associated with an increase of approximately 0.79 patents.\n\nThis suggests that Blueprinty’s software has a substantial and positive effect on firms’ innovation output, as measured by patent generation."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\ndf.info()\ndf.describe(include=\"all\")\ndf.isnull().sum()\nplt.figure(figsize=(8, 4))\nplt.hist(df['patents'], bins=30)\nplt.title('Distribution of Patents Awarded')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.histplot(data=df, x='patents', hue='iscustomer', bins=30, multiple='dodge')\nplt.title('Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.legend(title='Uses Blueprinty', labels=['No', 'Yes'])\nplt.show()\ndf.groupby('iscustomer')['patents'].mean()\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nFrom the histograms, we observe that firms using Blueprinty’s software tend to have a distribution shifted slightly to the right compared to those not using the software, suggesting higher patent counts.\nThe mean number of patents is higher among customers of Blueprinty than non-customers, which may suggest an association between using the software and patenting success. However, further modeling is required to control for other factors like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport numpy as np\n\nplt.figure(figsize=(10, 5))\n\nbins = np.arange(8, 51, 1) \n\nplt.hist(df[df['iscustomer'] == 0]['age'], bins=bins - 0.2, width=0.4, alpha=0.7, label='Not Using Blueprinty', color='skyblue')\nplt.hist(df[df['iscustomer'] == 1]['age'], bins=bins + 0.2, width=0.4, alpha=0.7, label='Using Blueprinty', color='orange')\n\nplt.title('Firm Age Distribution by Customer Status (Side-by-Side)')\nplt.xlabel('Firm Age')\nplt.ylabel('Number of Firms')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index')\nregion_counts.columns = ['Not Using', 'Using']\nregion_counts.plot(kind='bar', stacked=True, figsize=(8, 5))\nplt.title('Regional Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Proportion')\nplt.legend(title='Uses Blueprinty', loc='upper left', bbox_to_anchor=(1.02, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirms using Blueprinty software tend to be older, on average, than those not using it. Age might be a confounder and should be controlled for in any causal inference or regression analysis.\nThere are regional differences in Blueprinty software adoption, with the Northeast having a much higher proportion of customers. Region could be a significant predictor or confounder and should be included in modeling.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for the Poisson model is based on the assumption that the outcome variable \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\(\\lambda_i = \\exp(X_i \\beta)\\).\nThe probability mass function for a Poisson-distributed outcome is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations, the overall likelihood function for (n) firms is:\n\\[\nL(\\beta) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}, \\quad \\text{where } \\lambda_i = \\exp(X_i \\beta)\n\\]\nTaking the logarithm, the log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ -\\exp(X_i \\beta) + Y_i X_i \\beta - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n    \n    Parameters:\n        lmbda: array-like, predicted Poisson means (e.g. exp(X @ beta))\n        y: array-like, observed counts\n\n    Returns:\n        log-likelihood value (scalar)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\n\n\n\ny_obs_full = df['patents'].values\n\nlambda_range = np.linspace(0.1, 10, 200)\n\nlog_likelihoods_full = [poisson_loglikelihood(lmbda=np.repeat(lmb, len(y_obs_full)), y=y_obs_full)\n                        for lmb in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, log_likelihoods_full)\nplt.title('Poisson Log-Likelihood vs Lambda (Full Sample)')\nplt.xlabel('Lambda (shared across firms)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) of \\(\\lambda\\), we start from the Poisson log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean is the natural estimate of the population mean under the MLE framework.\n\nlambda_mle = df['patents'].mean()\nprint(f\"Lambda MLE (sample mean of Y): {lambda_mle:.4f}\")\n\nLambda MLE (sample mean of Y): 3.6847\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndef neg_log_likelihood(lmbda_scalar, y):\n    lmbda = np.repeat(lmbda_scalar[0], len(y))\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny_data = df['patents'].values\n\ninitial_guess = [np.mean(y_data)]\n\nresult = minimize(neg_log_likelihood, x0=initial_guess, args=(y_data,), bounds=[(0.0001, None)])\n\nlambda_mle = result.x[0]\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef neg_poisson_loglikelihood(beta, X, y):\n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n\n    linear_pred = X @ beta\n    lmbda = np.exp(linear_pred)\n    log_lik = -lmbda + y * linear_pred - gammaln(y + 1)\n    return -np.sum(log_lik)\n\n\n\n\nfrom scipy.optimize import minimize\n\nX = df[['age']].copy()\nX['age_squared'] = X['age']**2\nX['iscustomer'] = df['iscustomer']\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float)\nX = X.values\n\ny = df['patents'].values\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 1000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3269.138673\n         Iterations: 376\n         Function evaluations: 588\nEstimated beta coefficients:\n[ 0.11541318 -0.00237274  0.10391813  0.02789235 -0.07511018 -0.01228066\n -0.05693624]\n\n\nThe Poisson regression model estimated the impact of firm characteristics on patent counts. We find that firm age is positively associated with patent output, while being a Blueprinty customer is also linked to a higher expected number of patents. Regional effects are small and mixed, with most being slightly negative relative to the omitted baseline region.\n\nX = df[['age']].copy()\nX['age_squared'] = X['age'] ** 2\nX['iscustomer'] = df['iscustomer']\n\nX.insert(0, 'intercept', 1)\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float).values\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 2000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3259.214530\n         Iterations: 685\n         Function evaluations: 1054\nEstimated beta coefficients:\n[-0.32950681  0.13700644 -0.00276358  0.21449006 -0.01353866 -0.05320079\n  0.02214911  0.04299568]\n\n\n\nimport pandas as pd\nimport numdifftools as nd\n\nhessian_fun = nd.Hessian(lambda b: neg_poisson_loglikelihood(b, X, y))\nH = hessian_fun(beta_mle)\n\ncov_matrix = np.linalg.inv(H)\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\ncolumns = ['intercept', 'age', 'age_squared', 'iscustomer'] + list(pd.get_dummies(df['region'], drop_first=True).columns)\n\nresults_table = pd.DataFrame({\n    'Variable': columns,\n    'Beta': beta_mle,\n    'Std. Error': standard_errors\n})\n\nresults_table\n\n/var/folders/32/xz9pd3zx1yv2tk_19psx52z40000gn/T/ipykernel_25408/820769554.py:11: RuntimeWarning: overflow encountered in exp\n  lmbda = np.exp(linear_pred)\n\n\n\n\n\n\n\n\n\nVariable\nBeta\nStd. Error\n\n\n\n\n0\nintercept\n-0.329507\n6.520750e-02\n\n\n1\nage\n0.137006\n2.057879e-03\n\n\n2\nage_squared\n-0.002764\n1.473287e-09\n\n\n3\niscustomer\n0.214490\n3.075139e-02\n\n\n4\nNortheast\n-0.013539\n4.321331e-02\n\n\n5\nNorthwest\n-0.053201\n5.356145e-02\n\n\n6\nSouth\n0.022149\n5.239738e-02\n\n\n7\nSouthwest\n0.042996\n4.660574e-02\n\n\n\n\n\n\n\nThe inclusion of a constant term (intercept = -0.3295) ensures the model accounts for a baseline patent rate when all covariates are zero.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_check = df[['age']].copy()\nX_check['age_squared'] = X_check['age']**2\nX_check['iscustomer'] = df['iscustomer']\nX_check = pd.concat([X_check, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX_check = sm.add_constant(X_check) \nX_check = X_check.astype(float)\n\nglm_poisson = sm.GLM(df['patents'], X_check, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\n\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:28:33   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\n\nThe results are mostly consistent, with some small differences in coefficient values (possibly due to different scaling, convergence settings, or data preprocessing).\n\n\n\n\nThis analysis uses a Poisson regression model to examine the factors influencing the number of patents. The model is specified as a Generalized Linear Model (GLM) with a log link and Poisson distribution.\n\n\n\nDependent Variable: patents (number of patents)\nSample Size: 1500 observations\n\nPseudo R-squared: 0.1360 (indicates moderate explanatory power)\n\nLink Function: Log\n\nFitting Method: IRLS (Iteratively Reweighted Least Squares)\n\n\n\n\n\nBelow is the interpretation of key coefficients:\nIntercept (const = -0.5089)\nWhen all predictors are set to zero, the expected log count of patents is -0.5089. Converting from log to count scale: exp(-0.5089) ≈ 0.60 → the baseline expected number of patents is approximately 0.60.\nAge (coef = 0.1486, p &lt; 0.001)\nEach additional year of age increases the log of the expected patent count by 0.1486. On the count scale: exp(0.1486) ≈ 1.16, meaning a 16% increase in the expected number of patents per additional year of age.\nAge Squared (coef = -0.0030, p &lt; 0.001)\nThe negative coefficient indicates a concave (inverted-U) relationship between age and patents: the number of patents increases with age up to a certain point, then starts to decline.\nIs Customer (coef = 0.2076, p &lt; 0.001)\nBeing a customer increases the expected number of patents. exp(0.2076) ≈ 1.23 → customers are expected to have 23% more patents than non-customers.\nRegion Variables (Northeast, Northwest, South, Southwest)\nNone of these regional indicators are statistically significant (p-values &gt; 0.05), suggesting no strong evidence that geographic region affects patent counts in this model.\nIn summary, Age has a significant nonlinear effect on patent count. Customer status is a strong and significant positive predictor. Regional location appears not to significantly affect patent outcomes. The model captures meaningful patterns, though its overall explanatory power is moderate (Pseudo R² = 0.136)\n\n\n\nTo understand the effect of using Blueprinty’s software (represented by the iscustomer variable) on patent success, we estimate the average treatment effect (ATE). Since we are using a Poisson regression with a log link, the beta coefficient for iscustomer is not directly interpretable in units of patent count.\nInstead, we use a counterfactual prediction approach:\n\nCreate two copies of the dataset:\n\nX_0: with iscustomer = 0 for all firms (non-customers)\nX_1: with iscustomer = 1 for all firms (customers)\n\nUse the fitted model to generate predicted patent counts for each dataset:\n\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\nCompute the individual treatment effects (y_pred_1 - y_pred_0), and take their average.\n\n\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\n# Assuming X and y are your data\nX_df = pd.DataFrame(X, columns=[\n    'intercept', 'age', 'age_squared', 'iscustomer',\n    'Northeast', 'Northwest', 'South', 'Southwest'\n])\n\n# Fit the model if it's not already done\nmodel = sm.GLM(y, X_df, family=sm.families.Poisson(link=sm.families.links.log())).fit()\n\n# Now create counterfactuals\nX_0 = X_df.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X_df.copy()\nX_1['iscustomer'] = 1\n\n# Predict\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\n# Treatment effect\ntreatment_effect = y_pred_1 - y_pred_0\naverage_effect = treatment_effect.mean()\n\naverage_effect\n\n/Users/wenxinxu/Library/Python/3.9/lib/python/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n  warnings.warn(\n\n\nnp.float64(0.7927680710452972)\n\n\n\n\n\nOn average, being a Blueprinty customer is associated with an increase of approximately 0.79 patents.\n\nThis suggests that Blueprinty’s software has a substantial and positive effect on firms’ innovation output, as measured by patent generation."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe treat the number of reviews as a proxy for booking volume, based on the assumption that more bookings lead to more reviews. ### Data Overview\n\nimport pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Summary statistics and missing value inspection\ndf.describe()\ndf.isna().mean().sort_values(ascending=False)\n\nreview_scores_value          0.252437\nreview_scores_location       0.252388\nreview_scores_cleanliness    0.250935\nbathrooms                    0.003938\nbedrooms                     0.001871\nhost_since                   0.000861\nUnnamed: 0                   0.000000\nid                           0.000000\ndays                         0.000000\nlast_scraped                 0.000000\nroom_type                    0.000000\nprice                        0.000000\nnumber_of_reviews            0.000000\ninstant_bookable             0.000000\ndtype: float64\n\n\n\ndf_clean = df.dropna(subset=[\n    'review_scores_value', \n    'review_scores_location', \n    'review_scores_cleanliness'\n])\n\n\n\nModeling: Poisson Regression\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Poisson regression model\nmodel = smf.glm(\n    formula=\"number_of_reviews ~ price + C(room_type) + review_scores_cleanliness + review_scores_location + review_scores_value + C(instant_bookable)\",\n    data=df_clean,\n    family=sm.families.Poisson()\n).fit()\n\nmodel.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30346\n\n\nModel:\nGLM\nDf Residuals:\n30338\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.3507e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n9.4783e+05\n\n\nTime:\n20:28:34\nPearson chi2:\n1.42e+06\n\n\nNo. Iterations:\n6\nPseudo R-squ. (CS):\n0.5380\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n3.5679\n0.015\n233.369\n0.000\n3.538\n3.598\n\n\nC(room_type)[T.Private room]\n-0.0252\n0.003\n-9.406\n0.000\n-0.031\n-0.020\n\n\nC(room_type)[T.Shared room]\n-0.2648\n0.009\n-30.926\n0.000\n-0.282\n-0.248\n\n\nC(instant_bookable)[T.t]\n0.3324\n0.003\n115.542\n0.000\n0.327\n0.338\n\n\nprice\n-7.422e-06\n7.57e-06\n-0.980\n0.327\n-2.23e-05\n7.42e-06\n\n\nreview_scores_cleanliness\n0.1130\n0.001\n76.151\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0821\n0.002\n-51.797\n0.000\n-0.085\n-0.079\n\n\nreview_scores_value\n-0.0900\n0.002\n-50.354\n0.000\n-0.094\n-0.087\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe fit a Poisson regression model to examine how listing characteristics influence the number of Airbnb reviews, treating number_of_reviews as a proxy for booking activity. Below is the interpretation of the model coefficients:\n\nKey Findings\n\nRoom Type\n\nPrivate room: Coefficient = -0.0252, p &lt; 0.001\n→ Listings that are private rooms receive approximately 2.5% fewer reviews than entire apartments, holding other factors constant.\n→ exp(-0.0252) ≈ 0.975\nShared room: Coefficient = -0.2648, p &lt; 0.001\n→ Shared rooms receive approximately 23.3% fewer reviews than entire apartments.\n→ exp(-0.2648) ≈ 0.767\n\nInstant Bookable\n\nCoefficient = 0.3324, p &lt; 0.001\n→ Listings that are instantly bookable receive about 39.4% more reviews than those requiring host approval.\n→ exp(0.3324) ≈ 1.394\n\nPrice\n\nCoefficient = -7.42e-06, p = 0.327 (not significant)\n→ Price does not significantly influence the number of reviews in this model.\n\nReview Scores\n\nCleanliness: Coefficient = 0.1130, p &lt; 0.001\n→ A one-point increase in cleanliness rating is associated with a ~12% increase in reviews.\n→ exp(0.1130) ≈ 1.12\nLocation: Coefficient = -0.0821, p &lt; 0.001\n→ Surprisingly, higher location ratings are associated with ~7.9% fewer reviews.\n→ This may reflect an inverse relationship between satisfaction and motivation to leave a review.\nValue: Coefficient = -0.0900, p &lt; 0.001\n→ Higher value scores also correspond to ~8.6% fewer reviews.\n→ exp(-0.0900) ≈ 0.914\n\n\n\n\n\nModel Fit\n\nPseudo R² = 0.538\n→ The model explains approximately 53.8% of the variance in the (log) number of reviews, suggesting good overall fit.\n\n\n\nConclusion\nThe analysis reveals that room type, booking convenience, and review scores play a significant role in predicting review volume. Notably, cleanliness and instant bookability are strong positive drivers, while price has no significant impact.\n\n\nNegative Binomial Regression\nTo account for potential overdispersion in the count data (number_of_reviews), we fit a Negative Binomial regression model. This is more flexible than Poisson regression when the variance exceeds the mean.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit the model (formula same as in Poisson)\nnb_model = smf.glm(\n    formula=\"number_of_reviews ~ price + C(room_type) + C(instant_bookable) + review_scores_cleanliness + review_scores_location + review_scores_value\",\n    data=df_clean,\n    family=sm.families.NegativeBinomial()\n).fit()\n\nnb_model.summary()\n\n/Users/wenxinxu/Library/Python/3.9/lib/python/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30346\n\n\nModel:\nGLM\nDf Residuals:\n30338\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1.2313e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n48337.\n\n\nTime:\n20:28:34\nPearson chi2:\n6.51e+04\n\n\nNo. Iterations:\n21\nPseudo R-squ. (CS):\n0.04292\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n4.2213\n0.077\n55.167\n0.000\n4.071\n4.371\n\n\nC(room_type)[T.Private room]\n-0.0054\n0.013\n-0.429\n0.668\n-0.030\n0.019\n\n\nC(room_type)[T.Shared room]\n-0.2298\n0.037\n-6.269\n0.000\n-0.302\n-0.158\n\n\nC(instant_bookable)[T.t]\n0.3231\n0.015\n21.776\n0.000\n0.294\n0.352\n\n\nprice\n1.711e-05\n3.3e-05\n0.519\n0.604\n-4.75e-05\n8.17e-05\n\n\nreview_scores_cleanliness\n0.1959\n0.007\n28.924\n0.000\n0.183\n0.209\n\n\nreview_scores_location\n-0.1167\n0.008\n-14.841\n0.000\n-0.132\n-0.101\n\n\nreview_scores_value\n-0.2088\n0.009\n-23.724\n0.000\n-0.226\n-0.192\n\n\n\n\n\n\n\nInterpretation of Negative Binomial Regression Results\nTo account for potential overdispersion in the count of reviews, we fitted a Negative Binomial regression model. The dependent variable is number_of_reviews, used as a proxy for booking frequency.\n\nKey Findings\n\nRoom Type\n\nPrivate room: Coefficient = -0.0054, p = 0.668\n→ Not statistically significant. Private rooms receive a similar number of reviews as entire apartments.\nShared room: Coefficient = -0.2298, p &lt; 0.001\n→ Shared rooms receive ~20.5% fewer reviews than entire apartments.\n→ exp(-0.2298) ≈ 0.795\n\nInstant Bookable\n\nCoefficient = 0.3231, p &lt; 0.001\n→ Instantly bookable listings receive ~38.1% more reviews.\n→ exp(0.3231) ≈ 1.381\n\nPrice\n\nCoefficient = 1.71e-05, p = 0.604\n→ Not statistically significant. Price does not appear to affect the number of reviews.\n\nReview Scores\n\nCleanliness: Coefficient = 0.1959, p &lt; 0.001\n→ A one-point increase in cleanliness rating is associated with a ~21.6% increase in expected reviews.\n→ exp(0.1959) ≈ 1.216\nLocation: Coefficient = -0.1167, p &lt; 0.001\n→ Higher location scores are unexpectedly associated with ~11% fewer reviews.\n→ exp(-0.1167) ≈ 0.89\nValue: Coefficient = -0.2088, p &lt; 0.001\n→ Higher value scores are associated with ~18.8% fewer reviews.\n→ exp(-0.2088) ≈ 0.812\n\n\n\n\n\nModel Performance\n\nPseudo R² (CS) = 0.0429\n→ The model explains about 4.3% of the variation in review count — a lower fit compared to Poisson, but potentially more appropriate due to overdispersion.\n\n\n\nConclusion\nThis model confirms that: - Cleanliness and instant bookability are strong positive drivers of review (and likely booking) volume. - The effect of room type is significant only for shared rooms. - Price does not influence review count. - Higher review scores for value and location are surprisingly associated with fewer reviews, possibly reflecting inverse satisfaction-review behavior or confounding factors.\n\n\nComparing Poisson and Negative Binomial Models\n\nmean_reviews = df_clean['number_of_reviews'].mean()\nvar_reviews = df_clean['number_of_reviews'].var()\nprint(f\"Mean: {mean_reviews}, Variance: {var_reviews}\")\n\nprint(\"Poisson AIC:\", model.aic)\nprint(\"Negative Binomial AIC:\", nb_model.aic)\n\noverdispersion = var_reviews &gt; mean_reviews * 1.5 \nprint(\"Overdispersion?\", overdispersion)\n\nMean: 21.252323205694324, Variance: 1031.7342877315536\nPoisson AIC: 1070158.6878802574\nNegative Binomial AIC: 246285.75583628635\nOverdispersion? True\n\n\n\n\nModel Comparison: Poisson vs. Negative Binomial\nTo determine which model better fits the review count data, we compared the Poisson and Negative Binomial regression models using AIC and checked for overdispersion.\n\nKey Metrics:\n\nMean number of reviews: 21.25\n\nVariance of reviews: 1031.73\n\nPoisson AIC: 1,070,158.69\n\nNegative Binomial AIC: 246,285.76\n\nOverdispersion detected?: True\n\n\n\n\nInterpretation:\n\nThe variance far exceeds the mean (1031 vs. 21), indicating strong overdispersion. This violates the Poisson model’s key assumption that variance equals the mean.\nAlthough the Poisson model reported a high pseudo R², its extremely high AIC shows that it is likely overfitting and inappropriate for the data.\nThe Negative Binomial model has a much lower AIC, confirming it better fits the overdispersed data—even though its pseudo R² is lower, it’s more statistically sound and trustworthy.\n\n\n\nSummary:\n\nGiven the presence of overdispersion and a significantly lower AIC, the Negative Binomial model is the preferred choice for modeling the number of Airbnb reviews in this dataset."
  },
  {
    "objectID": "projects/project2/Untitled-1.html",
    "href": "projects/project2/Untitled-1.html",
    "title": "Wendy",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('projects/project2/blueprinty.csv')\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reload the dataset\ndf = pd.read_csv(\"projects/project2/blueprinty.csv\")\n\n# Set up the combined histogram + KDE plot\nplt.figure(figsize=(10, 5))\n\n# Histogram and KDE for non-customers\nsns.histplot(df[df['iscustomer'] == 0]['age'],\n             bins=30,\n             kde=True,\n             stat=\"density\",\n             label='Not Using Blueprinty',\n             color='skyblue',\n             alpha=0.5)\n\n\n\n\n\n\n\n\n\nage_distribution_non_customers = df[df['iscustomer'] == 0]['age'].value_counts().sort_index()\nage_distribution_non_customers\n\nage\n9.0     1\n9.5     1\n10.0    2\n11.0    1\n11.5    3\n       ..\n43.0    1\n43.5    1\n44.0    2\n46.5    1\n47.5    1\nName: count, Length: 72, dtype: int64"
  },
  {
    "objectID": "projects/project2/hw2_questions.html#log-likelihood-curve-for-poisson-model",
    "href": "projects/project2/hw2_questions.html#log-likelihood-curve-for-poisson-model",
    "title": "Poisson Regression Examples",
    "section": "Log-Likelihood Curve for Poisson Model",
    "text": "Log-Likelihood Curve for Poisson Model\n\ny_obs_full = df['patents'].values\n\nlambda_range = np.linspace(0.1, 10, 200)\n\nlog_likelihoods_full = [poisson_loglikelihood(lmbda=np.repeat(lmb, len(y_obs_full)), y=y_obs_full)\n                        for lmb in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, log_likelihoods_full)\nplt.title('Poisson Log-Likelihood vs Lambda (Full Sample)')\nplt.xlabel('Lambda (shared across firms)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\nAnalytical Derivation of MLE for Lambda\nTo find the maximum likelihood estimator (MLE) of \\(\\lambda\\), we start from the Poisson log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean is the natural estimate of the population mean under the MLE framework.\n\nlambda_mle = df['patents'].mean()\nprint(f\"Lambda MLE (sample mean of Y): {lambda_mle:.4f}\")\n\nLambda MLE (sample mean of Y): 3.6847\n\n\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#maximum-likelihood-estimation-via-optimization",
    "href": "projects/project2/hw2_questions.html#maximum-likelihood-estimation-via-optimization",
    "title": "Poisson Regression Examples",
    "section": "Maximum Likelihood Estimation via Optimization",
    "text": "Maximum Likelihood Estimation via Optimization\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndef neg_log_likelihood(lmbda_scalar, y):\n    lmbda = np.repeat(lmbda_scalar[0], len(y))\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny_data = df['patents'].values\n\ninitial_guess = [np.mean(y_data)]\n\nresult = minimize(neg_log_likelihood, x0=initial_guess, args=(y_data,), bounds=[(0.0001, None)])\n\nlambda_mle = result.x[0]\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}"
  },
  {
    "objectID": "projects/project2/hw2_questions.html#poisson-regression-log-likelihood",
    "href": "projects/project2/hw2_questions.html#poisson-regression-log-likelihood",
    "title": "Poisson Regression Examples",
    "section": "Poisson Regression Log-Likelihood",
    "text": "Poisson Regression Log-Likelihood\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef neg_poisson_loglikelihood(beta, X, y):\n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n\n    linear_pred = X @ beta\n    lmbda = np.exp(linear_pred)\n    log_lik = -lmbda + y * linear_pred - gammaln(y + 1)\n    return -np.sum(log_lik)\n\n\nEstimate Beta via MLE\n\nfrom scipy.optimize import minimize\n\nX = df[['age']].copy()\nX['age_squared'] = X['age']**2\nX['iscustomer'] = df['iscustomer']\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float)\nX = X.values\n\ny = df['patents'].values\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 1000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3269.138673\n         Iterations: 376\n         Function evaluations: 588\nEstimated beta coefficients:\n[ 0.11541318 -0.00237274  0.10391813  0.02789235 -0.07511018 -0.01228066\n -0.05693624]\n\n\nThe Poisson regression model estimated the impact of firm characteristics on patent counts. We find that firm age is positively associated with patent output, while being a Blueprinty customer is also linked to a higher expected number of patents. Regional effects are small and mixed, with most being slightly negative relative to the omitted baseline region.\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\nX = df[['age']].copy()\nX['age_squared'] = X['age'] ** 2\nX['iscustomer'] = df['iscustomer']\n\nX.insert(0, 'intercept', 1)\n\nX = pd.concat([X, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX = X.astype(float).values\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ninit_beta = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=neg_poisson_loglikelihood,\n    x0=init_beta,\n    args=(X, y),\n    method='Nelder-Mead',\n    options={'disp': True, 'maxiter': 2000}\n)\n\nbeta_mle = res.x\nprint(\"Estimated beta coefficients:\")\nprint(beta_mle)\n\nOptimization terminated successfully.\n         Current function value: 3259.214530\n         Iterations: 685\n         Function evaluations: 1054\nEstimated beta coefficients:\n[-0.32950681  0.13700644 -0.00276358  0.21449006 -0.01353866 -0.05320079\n  0.02214911  0.04299568]\n\n\n\nimport pandas as pd\nimport numdifftools as nd\n\nhessian_fun = nd.Hessian(lambda b: neg_poisson_loglikelihood(b, X, y))\nH = hessian_fun(beta_mle)\n\ncov_matrix = np.linalg.inv(H)\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\ncolumns = ['intercept', 'age', 'age_squared', 'iscustomer'] + list(pd.get_dummies(df['region'], drop_first=True).columns)\n\nresults_table = pd.DataFrame({\n    'Variable': columns,\n    'Beta': beta_mle,\n    'Std. Error': standard_errors\n})\n\nresults_table\n\n/var/folders/32/xz9pd3zx1yv2tk_19psx52z40000gn/T/ipykernel_24552/820769554.py:11: RuntimeWarning: overflow encountered in exp\n  lmbda = np.exp(linear_pred)\n\n\n\n\n\n\n\n\n\nVariable\nBeta\nStd. Error\n\n\n\n\n0\nintercept\n-0.329507\n6.520750e-02\n\n\n1\nage\n0.137006\n2.057879e-03\n\n\n2\nage_squared\n-0.002764\n1.473287e-09\n\n\n3\niscustomer\n0.214490\n3.075139e-02\n\n\n4\nNortheast\n-0.013539\n4.321331e-02\n\n\n5\nNorthwest\n-0.053201\n5.356145e-02\n\n\n6\nSouth\n0.022149\n5.239738e-02\n\n\n7\nSouthwest\n0.042996\n4.660574e-02\n\n\n\n\n\n\n\nThe inclusion of a constant term (intercept = -0.3295) ensures the model accounts for a baseline patent rate when all covariates are zero.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_check = df[['age']].copy()\nX_check['age_squared'] = X_check['age']**2\nX_check['iscustomer'] = df['iscustomer']\nX_check = pd.concat([X_check, pd.get_dummies(df['region'], drop_first=True)], axis=1)\nX_check = sm.add_constant(X_check) \nX_check = X_check.astype(float)\n\nglm_poisson = sm.GLM(df['patents'], X_check, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\n\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        20:06:40   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\n\nThe results are mostly consistent, with some small differences in coefficient values (possibly due to different scaling, convergence settings, or data preprocessing).\ntodo: Interpret the results."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#interpretation-of-regression-results",
    "href": "projects/project2/hw2_questions.html#interpretation-of-regression-results",
    "title": "Poisson Regression Examples",
    "section": "Interpretation of Regression Results",
    "text": "Interpretation of Regression Results\nThis analysis uses a Poisson regression model to examine the factors influencing the number of patents. The model is specified as a Generalized Linear Model (GLM) with a log link and Poisson distribution.\n\nModel Overview\n\nDependent Variable: patents (number of patents)\nSample Size: 1500 observations\n\nPseudo R-squared: 0.1360 (indicates moderate explanatory power)\n\nLink Function: Log\n\nFitting Method: IRLS (Iteratively Reweighted Least Squares)\n\n\n\nKey Coefficient Interpretations\nBelow is the interpretation of key coefficients:\nIntercept (const = -0.5089)\nWhen all predictors are set to zero, the expected log count of patents is -0.5089. Converting from log to count scale: exp(-0.5089) ≈ 0.60 → the baseline expected number of patents is approximately 0.60.\nAge (coef = 0.1486, p &lt; 0.001)\nEach additional year of age increases the log of the expected patent count by 0.1486. On the count scale: exp(0.1486) ≈ 1.16, meaning a 16% increase in the expected number of patents per additional year of age.\nAge Squared (coef = -0.0030, p &lt; 0.001)\nThe negative coefficient indicates a concave (inverted-U) relationship between age and patents: the number of patents increases with age up to a certain point, then starts to decline.\nIs Customer (coef = 0.2076, p &lt; 0.001)\nBeing a customer increases the expected number of patents. exp(0.2076) ≈ 1.23 → customers are expected to have 23% more patents than non-customers.\nRegion Variables (Northeast, Northwest, South, Southwest)\nNone of these regional indicators are statistically significant (p-values &gt; 0.05), suggesting no strong evidence that geographic region affects patent counts in this model.\nIn summary, Age has a significant nonlinear effect on patent count. Customer status is a strong and significant positive predictor. Regional location appears not to significantly affect patent outcomes. The model captures meaningful patterns, though its overall explanatory power is moderate (Pseudo R² = 0.136)\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#estimating-the-effect-of-blueprintys-software-on-patent-output",
    "href": "projects/project2/hw2_questions.html#estimating-the-effect-of-blueprintys-software-on-patent-output",
    "title": "Poisson Regression Examples",
    "section": "Estimating the Effect of Blueprinty’s Software on Patent Output",
    "text": "Estimating the Effect of Blueprinty’s Software on Patent Output\nTo understand the effect of using Blueprinty’s software (represented by the iscustomer variable) on patent success, we estimate the average treatment effect (ATE). Since we are using a Poisson regression with a log link, the beta coefficient for iscustomer is not directly interpretable in units of patent count.\nInstead, we use a counterfactual prediction approach:\n\nCreate two copies of the dataset:\n\nX_0: with iscustomer = 0 for all firms (non-customers)\nX_1: with iscustomer = 1 for all firms (customers)\n\nUse the fitted model to generate predicted patent counts for each dataset:\n\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\nCompute the individual treatment effects (y_pred_1 - y_pred_0), and take their average.\n\n\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\n# Assuming X and y are your data\nX_df = pd.DataFrame(X, columns=[\n    'intercept', 'age', 'age_squared', 'iscustomer',\n    'Northeast', 'Northwest', 'South', 'Southwest'\n])\n\n# Fit the model if it's not already done\nmodel = sm.GLM(y, X_df, family=sm.families.Poisson(link=sm.families.links.log())).fit()\n\n# Now create counterfactuals\nX_0 = X_df.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X_df.copy()\nX_1['iscustomer'] = 1\n\n# Predict\ny_pred_0 = model.predict(X_0)\ny_pred_1 = model.predict(X_1)\n\n# Treatment effect\ntreatment_effect = y_pred_1 - y_pred_0\naverage_effect = treatment_effect.mean()\n\naverage_effect\n\n/Users/wenxinxu/Library/Python/3.9/lib/python/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n  warnings.warn(\n\n\nnp.float64(0.7927680710452972)\n\n\n\nResult\n\nOn average, being a Blueprinty customer is associated with an increase of approximately 0.79 patents.\n\nThis suggests that Blueprinty’s software has a substantial and positive effect on firms’ innovation output, as measured by patent generation."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#effect-of-blueprintys-software-on-patent-output",
    "href": "projects/project2/hw2_questions.html#effect-of-blueprintys-software-on-patent-output",
    "title": "Poisson Regression Examples",
    "section": "Effect of Blueprinty’s Software on Patent Output",
    "text": "Effect of Blueprinty’s Software on Patent Output\nTo quantify the impact of Blueprinty’s software on patent success, we performed a counterfactual prediction exercise using our fitted Poisson regression model. Specifically, we estimated the expected number of patents for each firm under two scenarios:\n\nScenario 1: The firm is not a customer of Blueprinty (iscustomer = 0)\nScenario 2: The firm is a customer of Blueprinty (iscustomer = 1)\n\nWe computed the difference in predicted patent counts between the two scenarios for each firm and then averaged these differences across the dataset.\n\nResult\n\nOn average, being a Blueprinty customer is associated with an increase of approximately 0.79 patents.\n\nThis suggests that Blueprinty’s software has a substantial and positive effect on firms’ innovation output, as measured by patent generation."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#interpretation-of-poisson-regression-results",
    "href": "projects/project2/hw2_questions.html#interpretation-of-poisson-regression-results",
    "title": "Poisson Regression Examples",
    "section": "Interpretation of Poisson Regression Results",
    "text": "Interpretation of Poisson Regression Results\nWe fit a Poisson regression model to examine how listing characteristics influence the number of Airbnb reviews, treating number_of_reviews as a proxy for booking activity. Below is the interpretation of the model coefficients:\n\nKey Findings\n\nRoom Type\n\nPrivate room: Coefficient = -0.0252, p &lt; 0.001\n→ Listings that are private rooms receive approximately 2.5% fewer reviews than entire apartments, holding other factors constant.\n→ exp(-0.0252) ≈ 0.975\nShared room: Coefficient = -0.2648, p &lt; 0.001\n→ Shared rooms receive approximately 23.3% fewer reviews than entire apartments.\n→ exp(-0.2648) ≈ 0.767\n\nInstant Bookable\n\nCoefficient = 0.3324, p &lt; 0.001\n→ Listings that are instantly bookable receive about 39.4% more reviews than those requiring host approval.\n→ exp(0.3324) ≈ 1.394\n\nPrice\n\nCoefficient = -7.42e-06, p = 0.327 (not significant)\n→ Price does not significantly influence the number of reviews in this model.\n\nReview Scores\n\nCleanliness: Coefficient = 0.1130, p &lt; 0.001\n→ A one-point increase in cleanliness rating is associated with a ~12% increase in reviews.\n→ exp(0.1130) ≈ 1.12\nLocation: Coefficient = -0.0821, p &lt; 0.001\n→ Surprisingly, higher location ratings are associated with ~7.9% fewer reviews.\n→ This may reflect an inverse relationship between satisfaction and motivation to leave a review.\nValue: Coefficient = -0.0900, p &lt; 0.001\n→ Higher value scores also correspond to ~8.6% fewer reviews.\n→ exp(-0.0900) ≈ 0.914\n\n\n\n\nModel Fit\n\nPseudo R² = 0.538\n→ The model explains approximately 53.8% of the variance in the (log) number of reviews, suggesting good overall fit.\n\n\n\nConclusion\nThe analysis reveals that room type, booking convenience, and review scores play a significant role in predicting review volume. Notably, cleanliness and instant bookability are strong positive drivers, while price has no significant impact. todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]